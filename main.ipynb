{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  src.visualize_keypoints import *\n",
    "#from src.dataset import *\n",
    "import warnings\n",
    "import gdown\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file Dataset already exists.\n",
      "A subdirectory or file Output already exists.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir Dataset\n",
    "!mkdir Output\n",
    "#!gdown \"https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x\" -O Dataset --folder\n",
    "#!unzip Dataset/images.zip -d Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the following features:\n",
    "- **image_id :** (*input, int*) image identification, different animal samples on the same image share the same *image_id*.\n",
    "- **image :**(*input, array[1024*1024*3]*) image data of the sample.\n",
    "- **keypoints :** (*output, array[20*3]*) list of individual keypoints, which are lists of three values : [$x_{pos}$, $y_{pos}$, conf].\n",
    "- **bbox :** (*output, array[4]*) coordinates [$x$,$y$] for diagonal corners defining the bounding box of the animal.\n",
    "- **label :** (*output, int*) class id of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cow sheep horse cat dog\n",
    "labels = ['dog', 'cat', 'sheep', 'horse', 'cow']\n",
    "\n",
    "class AnimalPoseDataset(Dataset):\n",
    "    def __init__ (self, json_file, root_dir, transform=None):\n",
    "        self.keypoints_frame = json.load(open(json_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keypoints_frame[\"annotations\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        anno_dict = self.keypoints_frame\n",
    "        img_id = anno_dict[\"annotations\"][idx][\"image_id\"]\n",
    "        image_map = anno_dict[\"images\"]\n",
    "        annotations = anno_dict[\"annotations\"]\n",
    "\n",
    "        imagename = image_map[str(annotations[idx][\"image_id\"])]\n",
    "        bbox = annotations[idx][\"bbox\"]\n",
    "        keypoints = annotations[idx][\"keypoints\"]\n",
    "        label = labels[annotations[idx][\"category_id\"]-1]\n",
    "        image_path = os.path.join(self.root_dir, imagename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        sample = {'image_id': img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def draw(self, sample):\n",
    "        image = sample['image']\n",
    "        bbox = sample['bbox']\n",
    "        xmin, ymin, xmax, ymax = bbox \n",
    "        image = draw_bbox(image, xmin, ymin, xmax, ymax, random_color())\n",
    "        image = draw_keypoint(image, sample['keypoints'])\n",
    "        return image\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "  return result\n",
    "\n",
    "class Rescale (object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__ (self, sample):\n",
    "        img_id, image, keypoints, bbox = sample['image_id'],sample['image'], sample['keypoints'], sample['bbox']\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        # scale the image\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        # scale the keypoints\n",
    "        scaled_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            new_x = int(kp[0] * new_w / w)\n",
    "            new_y = int(kp[1] * new_h / h)\n",
    "            scaled_keypoints.append([new_x, new_y, kp[2]])\n",
    "        # scale the bbox\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        xmin = int(xmin * new_w / w)\n",
    "        xmax = int(xmax * new_w / w)\n",
    "        ymin = int(ymin * new_h / h)\n",
    "        ymax = int(ymax * new_h / h)\n",
    "        bbox = [xmin, ymin, xmax, ymax]\n",
    "        \n",
    "\n",
    "        return {'image_id':img_id, 'image': img, 'keypoints': scaled_keypoints, 'bbox':bbox, 'label':sample['label']}\n",
    "        \n",
    "class SDA(object):\n",
    "    \n",
    "    def __init__(self, nb_bodyparts, tolerance=20):\n",
    "        # number of body parts to add to the image\n",
    "        self.nb_bodyparts = nb_bodyparts\n",
    "        self.bodypart_pool = []\n",
    "        self.tolerance=tolerance\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_id, image, keypoints, bbox = sample['image_id'], sample['image'], sample['keypoints'], sample['bbox']\n",
    "        image, keypoints, bodyparts = self.crop_bodypart(image, keypoints)\n",
    "        self.bodypart_pool.extend(bodyparts)\n",
    "        \n",
    "        # add the body parts to the image\n",
    "        for i in range(self.nb_bodyparts):\n",
    "            image = self.add_bodyparts(image)\n",
    "        \n",
    "        return {'image_id':img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':sample['label']}\n",
    "\n",
    "    def crop_bodypart(self, image, keypoints):\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        draw_keypoint(mask, keypoints)\n",
    "        # find the contours in the mask\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # crop the different body parts and store them \n",
    "        bodyparts = []\n",
    "        for i in range(len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            bodyparts.append(image[y-self.tolerance:y+h+self.tolerance, x-self.tolerance:x+w+self.tolerance])\n",
    "        # return the image with the body parts and the keypoints\n",
    "        return image, keypoints, bodyparts\n",
    "    \n",
    "    def add_bodyparts(self, image):        \n",
    "        # randomly select a body part\n",
    "        # check if the body part pool is empty\n",
    "        if len(self.bodypart_pool) == 0:\n",
    "            return image\n",
    "        bodypart = random.choice(self.bodypart_pool)\n",
    "        # randomly select an angle\n",
    "        #angle = random.randint(0, 360)        \n",
    "        # rotate the body part\n",
    "        #bodypart = rotate_image(bodypart, angle)\n",
    "        h,w,_ = bodypart.shape\n",
    "\n",
    "        # randomly select a position for the body part\n",
    "        x = random.randint(0, image.shape[1] - w)\n",
    "        y = random.randint(0, image.shape[0] - h)\n",
    "        \n",
    "        image[y:y+h, x:x+w] = cv2.addWeighted(image[y:y+h, x:x+w], 0, bodypart, 1, 0)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def show_bodyparts(self):     \n",
    "        for i in range(len(self.bodypart_pool)):\n",
    "            plt.imshow(self.bodypart_pool[i])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 640, 640, 3]) ['dog', 'cow', 'dog', 'dog', 'dog', 'cat', 'cow', 'dog', 'dog', 'horse']\n",
      "1 torch.Size([10, 640, 640, 3]) ['cat', 'dog', 'cat', 'sheep', 'sheep', 'dog', 'horse', 'dog', 'cat', 'cat']\n",
      "2 torch.Size([10, 640, 640, 3]) ['cat', 'horse', 'cat', 'cat', 'cow', 'cow', 'cat', 'dog', 'dog', 'sheep']\n",
      "3 torch.Size([10, 640, 640, 3]) ['cat', 'cat', 'cat', 'dog', 'dog', 'cat', 'cat', 'cat', 'dog', 'cat']\n",
      "tensor([ 778, 1189, 2382,  445,  379,  579, 1155,  284,  739, 3396])\n",
      "keypoints:  [[tensor([440, 360, 474,   0, 481, 404,   0, 189, 292, 385]), tensor([249, 227, 478,   0, 225, 268,   0, 320, 192, 211]), tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1])], [tensor([371,   0, 503,   0, 276, 227,   0, 168, 186, 233]), tensor([234,   0, 362,   0, 224, 280,   0, 324, 158, 220]), tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1])], [tensor([400, 326, 463, 334, 440, 285,   0, 181, 184, 368]), tensor([301, 223, 406, 216, 352, 409,   0, 339, 229, 316]), tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])], [tensor([480, 410,   0,   0, 539, 473, 437, 188, 367, 377]), tensor([144, 223,   0,   0, 161, 120, 301, 281, 172,  90]), tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1])], [tensor([353, 389, 541, 348, 123, 202, 529, 148,   0,  87]), tensor([124, 189, 328, 275, 180, 126, 289, 290,   0, 104]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1])], [tensor([  0, 282, 239,   0,   0,   0,   0, 203, 301, 256]), tensor([  0, 265, 336,   0,   0,   0,   0, 329, 372, 446]), tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1])], [tensor([  0,   0, 285, 395,   0,   0,   0, 149, 134,   0]), tensor([  0,   0, 283, 343,   0,   0,   0, 352, 286,   0]), tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0])], [tensor([  0,   0,   0,   0,   0,   0,   0,   0, 427,   0]), tensor([  0,   0,   0,   0,   0,   0,   0,   0, 234,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])], [tensor([  0,   0, 211, 319,   0,   0,   0,   0,   0,   0]), tensor([  0,   0, 207, 432,   0,   0,   0,   0,   0,   0]), tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0])], [tensor([  0, 289, 221,   0,   0,   0,   0, 221, 231, 505]), tensor([  0, 219, 417,   0,   0,   0,   0, 355, 455, 545]), tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1])], [tensor([  0, 320, 285,   0,   0, 135,   0, 157, 115,   0]), tensor([  0, 203, 389,   0,   0, 512,   0, 363, 327,   0]), tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0])], [tensor([  0,   0,   0,   0,   0,   0,   0,   0, 425,   0]), tensor([  0,   0,   0,   0,   0,   0,   0,   0, 268,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])], [tensor([  0,   0,  83, 286,   0,   0,   0,   0,   0,   0]), tensor([  0,   0, 421, 500,   0,   0,   0,   0,   0,   0]), tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0])], [tensor([  0, 264, 144,   0,   0, 599,   0, 231, 170,   0]), tensor([  0, 167, 576,   0,   0, 216,   0, 366, 517,   0]), tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0])], [tensor([  0, 300, 249,   0,   0, 131,   0, 186,  87,   0]), tensor([  0, 173, 567,   0,   0, 585,   0, 371, 403,   0]), tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0])], [tensor([  0,   0, 378,   0,   0,   0,   0,   0, 423,   0]), tensor([  0,   0, 546,   0,   0,   0,   0,   0, 308,   0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])], [tensor([  0,   0, 172, 305,   0,   0,   0,   0,   0,   0]), tensor([  0,   0, 447, 531,   0,   0,   0,   0,   0,   0]), tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0])], [tensor([389, 322, 409,   0, 357,   0,   0,   0, 214,   0]), tensor([417, 281, 360,   0, 584,   0,   0,   0, 275,   0]), tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0])], [tensor([503, 394, 482, 314,   0,   0, 544,   0,   0,   0]), tensor([288, 349, 241, 317,   0,   0, 450,   0,   0,   0]), tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0])], [tensor([  0,   0,  37, 244,   0,   0,   0,   0, 425,   0]), tensor([  0,   0, 349, 426,   0,   0,   0,   0, 149,   0]), tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0])]]\n",
      "bbox : [tensor([311, 245,  21, 105,   1,   1, 403,  97,  55,   1]), tensor([ 46, 140,  38, 213,  14,   1, 251, 256,  94,   1]), tensor([640, 427, 615, 375, 640, 640, 640, 252, 575, 638]), tensor([547, 535, 618, 541, 640, 640, 640, 386, 540, 640])]\n",
      "label : ['cat', 'cat', 'cat', 'dog', 'dog', 'cat', 'cat', 'cat', 'dog', 'cat']\n",
      "torch.Size([10, 640, 640, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = AnimalPoseDataset(json_file='Dataset/keypoints.json', \n",
    "                            root_dir='Dataset/images/',\n",
    "                            transform=transforms.Compose([\n",
    "                                                        Rescale((640,640)),\n",
    "                                                        SDA(nb_bodyparts=3, tolerance=10)]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "sample = {'image_id':None, 'image': None, 'keypoints': None, 'bbox':None, 'label':None}\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(), sample_batched['label'])\n",
    "\n",
    "    if i_batch == 3:\n",
    "        sample['image_id'] = sample_batched['image_id']\n",
    "        sample['image'] = sample_batched['image']\n",
    "        sample['keypoints'] = sample_batched['keypoints']\n",
    "        sample['bbox'] = sample_batched['bbox']\n",
    "        sample['label'] = sample_batched['label']\n",
    "        break\n",
    "\n",
    "\n",
    "print(sample['image_id'])\n",
    "\n",
    "print(\"keypoints: \", sample['keypoints'])\n",
    "\n",
    "print(\"bbox :\", sample['bbox'])\n",
    "\n",
    "print(\"label :\",sample['label'])\n",
    "\n",
    "print(sample['image'].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **k** : kernel size\n",
    "- **s** : stride, determines the step size the convolution filter moves across the input image.\n",
    "- **p** : padding, used to control the spatial dimensions of the ouptut feature map by adding extra pixels around the input image or feature map before applying the convolution\n",
    "\n",
    "Special components:\n",
    "- **Split**: divides the input feature map into two or more separate feature maps along a specified axis(usually channel axis). This can be useful for processing parts of the feature map separately of feeding them into different parallel sub-nets\n",
    "- **Bottleneck**: design pattern often used to reduce the dimensionality of feature maps, followed by an expansion to the original dimensionality. This is usually achieved using a series of conv layers with varying kernels sizes and channel dimensions. Helps in reducing the models computational complexity while preserving relevant features\n",
    "- **Concat**: the concatenation op combines multiple feature maps along a specified axis. This is useful for merging information from different sources or resolutions withing the network, which can help improve the model's ability ot learn complex features and relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (100000.000000 --> 41444.148438).  Saving model ...\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 41444.148438 \tRemaining: 611\n",
      "Validation loss decreased (41444.148438 --> 38647.054688).  Saving model ...\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 38647.054688 \tRemaining: 610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 318\u001b[0m\n\u001b[0;32m    308\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCyclicLR(\n\u001b[0;32m    309\u001b[0m     optimizer, \n\u001b[0;32m    310\u001b[0m     base_lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m )\n\u001b[0;32m    317\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m--> 318\u001b[0m train_net(net, dataloader, n_epochs, optimizer, scheduler, criterion)\n",
      "Cell \u001b[1;32mIn[22], line 269\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(net, train_loader, n_epochs, optimizer, scheduler, criterion)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m# calculate the batch loss\u001b[39;00m\n\u001b[0;32m    268\u001b[0m bbox \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m loss1, _ \u001b[39m=\u001b[39m criterion(output[\u001b[39m1\u001b[39;49m], bbox)\n\u001b[0;32m    270\u001b[0m loss2,_ \u001b[39m=\u001b[39m criterion(output[\u001b[39m3\u001b[39m], bbox)\n\u001b[0;32m    271\u001b[0m loss3,_ \u001b[39m=\u001b[39m criterion(output[\u001b[39m5\u001b[39m], bbox)\n",
      "Cell \u001b[1;32mIn[22], line 233\u001b[0m, in \u001b[0;36mbbox_regression_loss\u001b[1;34m(pred_bboxes, gt_bboxes)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pred_bboxes_abs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    232\u001b[0m     \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pred_bboxes_abs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]):\n\u001b[1;32m--> 233\u001b[0m         iou \u001b[39m=\u001b[39m calculate_iou(pred_bboxes_abs[sample, :, bbox], gt_bboxes[sample, :])\n\u001b[0;32m    234\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m iou\n\u001b[0;32m    235\u001b[0m         iou_loss[sample, bbox] \u001b[39m=\u001b[39m loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Detail block modules\n",
    "# Conv\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, k, s, p, c_in, c_out):\n",
    "        super().__init__()\n",
    "        # 2d conv layer\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p)\n",
    "        # batch normalization\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        # SiLU activation\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, shortcut, h, w , c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in//2)\n",
    "        self.conv1 = Conv(k=3, s=1, p=1, c_in=c_in//2, c_out=c_in)\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shortcut:\n",
    "            xp = self.conv1(self.conv(x))\n",
    "            x = x + xp\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            x = self.conv1(x)        \n",
    "        return x\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=1, s=1, p=0, c_in=c_in,c_out=c_in)    \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=32, padding=2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.maxpool(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        xtot = x1 + x2 + x3 + x4\n",
    "        #x = torch.cat((x4, xtot), dim=1)\n",
    "        x = self.conv(xtot)\n",
    "        return x\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, shortcut, c_in, c_out, h, w, n):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(k=1, s=1, p=0, c_in=c_in, c_out=c_out)\n",
    "        self.conv2 = Conv(k=1, s=1, p=0, c_in=int(0.5*(n+2)*c_out), c_out=c_out)\n",
    "        self.bottleneck = Bottleneck(shortcut=shortcut, h=h, w=w, c_in=c_out//2)\n",
    "        self.n = n\n",
    "        self.cout = c_out\n",
    "        self.cin = c_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # split the input into half and the other half channels\n",
    "        split_x = torch.split(x, x.size(1)//2, dim=1)        \n",
    "        # current bottleneck\n",
    "        bn = split_x[0]\n",
    "        # list of bottlenecks to be stacked\n",
    "        bn_x = []\n",
    "        bn_x.append(bn)        \n",
    "        # iterate through the number of bottlenecks to be stacked\n",
    "        for i in range(self.n):\n",
    "            # apply the bottleneck to the first half channels and store them in bn_x\n",
    "            bn = self.bottleneck(bn)\n",
    "            bn_x.append(bn)\n",
    "               \n",
    "        # concatenate the first half channels with the second half channels\n",
    "        bn_x.append(split_x[1])\n",
    "        # concatenate the bottlenecks\n",
    "        x = torch.cat(bn_x, dim=1)\n",
    "        x = self.conv2(x)                \n",
    "        return x\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, reg_max, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in)\n",
    "        self.conv2d_bbox = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=4*reg_max)\n",
    "        self.conv2d_cls = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=num_classes)\n",
    "        self.linear = nn.Linear(in_features=6400, out_features=4)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv(x)\n",
    "        x_cls = self.conv2d_cls(x)\n",
    "        x_bbox = self.conv2d_bbox(x)\n",
    "        #x_bbox = self.calculate_bbox(x_bbox)\n",
    "        return x_cls, x_bbox\n",
    "\n",
    "    def calculate_bbox(self, x):\n",
    "        x_pred = torch.sigmoid(x[:,0,:, :])\n",
    "        y_pred = torch.sigmoid(x[:,1,:, :])\n",
    "        w_pred = torch.exp(x[:,2,:, :])\n",
    "        h_pred = torch.exp(x[:,3,:, :])\n",
    "        bbox = torch.cat([x_pred, y_pred, w_pred, h_pred], dim=1)\n",
    "        return bbox\n",
    "\n",
    "# Main Network architecture\n",
    "class BBoxNet(nn.Module):\n",
    "    def __init__(self, w, r, d):\n",
    "        super(BBoxNet, self).__init__()\n",
    "    # backbone network modules\n",
    "        self.conv_0_p1 = Conv(k=3, s=2, p=1, c_in=3, c_out=int(64*w))\n",
    "        self.conv_1_p2 = Conv(k=3, s=2, p=1, c_in=int(64*w), c_out=int(128*w))\n",
    "        self.c2f_2 = C2f(shortcut=True, h=160, w=160, n=int(3*d), c_in=int(128*w), c_out=int(128*w))\n",
    "        self.conv_3_p3 = Conv(k=3, s=2, p=1, c_in=int(128*w), c_out=int(256*w))\n",
    "        self.c2f_4 = C2f(shortcut=True, h=80, w=80, n=int(6*d), c_in=int(256*w), c_out=int(256*w))\n",
    "        self.conv_5_p4 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(512*w))\n",
    "        self.c2f_6 = C2f(shortcut=True, h=40, w=40, n=int(6*d), c_in=int(512*w), c_out=int(512*w))\n",
    "        self.conv_7_p5 = Conv(k=3, s=2, p=1, c_in=int(512*w), c_out=int(512*w*r))\n",
    "        self.c2f_8 = C2f(shortcut=True, h=20, w=20, n=int(3*d), c_in=int(512*w*r), c_out=int(512*r*w))\n",
    "        self.sppf_9 = SPPF(c_in=int(512*w*r))\n",
    "\n",
    "    # head network modules\n",
    "        self.upsample_10 = nn.Upsample(size=(40,40), mode='bilinear', align_corners=False)\n",
    "        self.concat_11 = torch.cat\n",
    "        self.c2f_12 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.upsample_resolution_13a = nn.Upsample(size=(80,80), mode='bilinear', align_corners=False)\n",
    "        self.upsample_channels_13b = nn.Conv2d(in_channels=int(512*w), out_channels=int(256*w), kernel_size=1)\n",
    "        self.concat_14 = torch.cat\n",
    "        self.c2f_15 = C2f(shortcut=False, c_in=int(512*w), c_out=int(256*w), h=80, w=80, n=int(3*d))\n",
    "        self.conv_16_p3 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(256*w))\n",
    "        self.concat_17 = torch.cat\n",
    "        # ISSUE HERE, THE ARCHITECTURE OUTPUT CHANNEL SIZE IS PROBABLY WRONG, AS THE CONCATENATION DOES NOT INCREASE THE CHANNEL SIZE\n",
    "        #self.c2f_18 = C2f(shortcut=False, c_in=int(512*w), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.c2f_18 = C2f(shortcut=False, c_in=192, c_out=192, h=40, w=40, n=int(3*d))\n",
    "        self.conv_19 = Conv(k=3, s=2, p=1, c_in=192, c_out=192)\n",
    "        self.concat_20 = torch.cat\n",
    "        #self.c2f_21 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "        self.c2f_21 = C2f(shortcut=False, c_in=448, c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "    \n",
    "    # output layers\n",
    "        self.detect1 = Detect(num_classes=6, reg_max=1, c_in=int(256*w))\n",
    "        self.detect2 = Detect(num_classes=6, reg_max=1, c_in=192)\n",
    "        self.detect3 = Detect(num_classes=6, reg_max=1, c_in=int(512*w))\n",
    "\n",
    "    def forward(self,x):\n",
    "    # backbone pass\n",
    "        x = self.conv_0_p1(x)\n",
    "        x = self.conv_1_p2(x)\n",
    "        x = self.c2f_2(x)\n",
    "        x = self.conv_3_p3(x)\n",
    "        x = self.c2f_4(x)\n",
    "        # save for concat later\n",
    "        x_4 = x\n",
    "        \n",
    "        x = self.conv_5_p4(x)\n",
    "        x = self.c2f_6(x)\n",
    "        \n",
    "        # save for concat later\n",
    "        x_6 = x\n",
    "        x = self.conv_7_p5(x)\n",
    "        x = self.c2f_8(x)\n",
    "        x = self.sppf_9(x)\n",
    "        x_9 = x\n",
    "\n",
    "    # head pass\n",
    "        # first brancH\n",
    "        x = self.upsample_10(x)\n",
    "        x = self.concat_11((x, x_6), dim=1)\n",
    "        x = self.c2f_12(x)  \n",
    "        x_12 = x\n",
    "        x = self.upsample_resolution_13a(x)\n",
    "        x = self.upsample_channels_13b(x)\n",
    "        x = self.concat_14((x, x_4), dim=1)\n",
    "        x = self.c2f_15(x) \n",
    "        x_detect1 = x\n",
    "        \n",
    "    # second branch\n",
    "        x = self.conv_16_p3(x)\n",
    "        # CHECK CHANNEL ISSUE HEREISSUE HERE\n",
    "        x = self.concat_17((x_12, x), dim=1)\n",
    "        x = self.c2f_18(x)\n",
    "        x_detect2 = x\n",
    "        x = self.conv_19(x)\n",
    "        # ISSUE PROPAGATES HERE ALSO\n",
    "        x = self.concat_20((x, x_9), dim=1)\n",
    "        x = self.c2f_21(x)\n",
    "        x_detect3 = x\n",
    "    \n",
    "    # output layers\n",
    "        x_cls1, x_bbox1 = self.detect1(x_detect1)\n",
    "        x_cls2, x_bbox2 = self.detect2(x_detect2)\n",
    "        x_cls3, x_bbox3 = self.detect3(x_detect3)        \n",
    "\n",
    "        return [x_cls1, x_bbox1, x_cls2, x_bbox2, x_cls3, x_bbox3]\n",
    "\n",
    "def calculate_iou(pred_bboxes, gt_bboxes):\n",
    "     # Calculate intersection\n",
    "    inter_xmin = torch.max(pred_bboxes[..., 0], gt_bboxes[..., 0])\n",
    "    inter_ymin = torch.max(pred_bboxes[..., 1], gt_bboxes[..., 1])\n",
    "    inter_xmax = torch.min(pred_bboxes[..., 2], gt_bboxes[..., 2])\n",
    "    inter_ymax = torch.min(pred_bboxes[..., 3], gt_bboxes[..., 3])\n",
    "\n",
    "    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
    "    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # Calculate union\n",
    "    pred_area = (pred_bboxes[..., 2] - pred_bboxes[..., 0]) * (pred_bboxes[..., 3] - pred_bboxes[..., 1])\n",
    "    gt_area = (gt_bboxes[..., 2] - gt_bboxes[..., 0]) * (gt_bboxes[..., 3] - gt_bboxes[..., 1])\n",
    "    union_area = pred_area + gt_area - inter_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "    \n",
    "def bbox_regression_loss(pred_bboxes, gt_bboxes):\n",
    "    # Reshape the predicted bounding boxes to (batch, 4, -1)\n",
    "    pred_bboxes = pred_bboxes.view(pred_bboxes.size(0), 4, -1)\n",
    "    # Convert the predicted bounding boxes to absolute coordinates\n",
    "    pred_bboxes_xy = torch.sigmoid(pred_bboxes[:, :2, :])\n",
    "    pred_bboxes_wh = torch.exp(pred_bboxes[:, 2:, :])\n",
    "    # Combine the x, y, width, and height to create the final predicted bounding boxes\n",
    "    pred_bboxes_abs = torch.cat((pred_bboxes_xy, pred_bboxes_wh), dim=1)\n",
    "    iou_loss = torch.zeros((pred_bboxes_abs.shape[0], pred_bboxes_abs.shape[2]))\n",
    "    id_list = torch.zeros((pred_bboxes_abs.shape[0], pred_bboxes_abs.shape[1]))\n",
    "\n",
    "    for sample in range(pred_bboxes_abs.shape[0]):\n",
    "        for bbox in range(pred_bboxes_abs.shape[2]):\n",
    "            iou = calculate_iou(pred_bboxes_abs[sample, :, bbox], gt_bboxes[sample, :])\n",
    "            loss = 1 - iou\n",
    "            iou_loss[sample, bbox] = loss\n",
    "\n",
    "        # get the index of the predicted bounding box with the highest IoU\n",
    "        max_iou, max_iou_idx = torch.max(iou_loss[sample], dim=0)\n",
    "        # if the IoU is greater than 0.5, then the predicted bounding box is a true positive\n",
    "        if max_iou > 0.5:\n",
    "            id_list[sample, :] = pred_bboxes_abs[sample, :, max_iou_idx]\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = torch.sum(torch.abs(id_list - gt_bboxes))\n",
    "    return loss, id_list\n",
    "\n",
    "    \n",
    "# define the training function\n",
    "def train_net(net, train_loader, n_epochs, optimizer, scheduler, criterion):\n",
    "    # loop over the number of epochs\n",
    "    best_loss = 100000\n",
    "    for epoch in range(n_epochs):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # set the model to training mode\n",
    "        net.train()\n",
    "        for i_batch, data in enumerate(dataloader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data = data.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = net(data['image'].permute(0,3,1,2).float())\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            bbox = torch.stack([t for t in data['bbox']], dim=1)\n",
    "            loss1, _ = criterion(output[1], bbox)\n",
    "            loss2,_ = criterion(output[3], bbox)\n",
    "            loss3,_ = criterion(output[5], bbox)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "\n",
    "            train_loss += loss.item()*data['image'].size(0)\n",
    "            # save the model if validation loss has decreased\n",
    "            if loss.item() < best_loss:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_loss, loss.item()))\n",
    "                torch.save(net.state_dict(), 'model.pt')\n",
    "                best_loss = loss.item()\n",
    "    \n",
    "            # save the three feature maps with different name        \n",
    "            if i_batch%10 == 0:\n",
    "                plt.imsave(\"Output/feature_map1_\"+str(i_batch)+\".jpg\", output[0][0,0,:,:].detach().cpu().numpy())\n",
    "                plt.imsave(\"Output/feature_map2_\"+str(i_batch)+\".jpg\", output[1][0,0,:,:].detach().cpu().numpy())\n",
    "                plt.imsave(\"Output/feature_map3_\"+str(i_batch)+\".jpg\", output[2][0,0,:,:].detach().cpu().numpy())\n",
    "\n",
    "            # print train loss in percentage, and remaining batches in epoch\n",
    "            print('Epoch: {} \\tBatch: {} \\tLoss: {:.6f} \\tRemaining: {}'.format(epoch+1, i_batch+1, loss.item(), len(train_loader)-i_batch-1))\n",
    "            \n",
    "\n",
    "        # print training statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "\n",
    "        # step the scheduler\n",
    "        scheduler.step()\n",
    "net = BBoxNet(w=0.25, r=2, d=0.34)\n",
    "net = net.to(device)\n",
    "criterion = bbox_regression_loss\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.002)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, \n",
    "    base_lr=0.00005, \n",
    "    max_lr=0.002,\n",
    "    step_size_up=10,\n",
    "    mode='exp_range',\n",
    "    cycle_momentum=False\n",
    ")\n",
    "n_epochs = 10\n",
    "train_net(net, dataloader, n_epochs, optimizer, scheduler, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  src.visualize_keypoints import *\n",
    "import warnings\n",
    "import gdown\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file Dataset already exists.\n",
      "A subdirectory or file Output already exists.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir Dataset\n",
    "!gdown \"https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x\" -O Dataset --folder\n",
    "!unzip Dataset/images.zip -d Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the following features:\n",
    "- **image_id :** (*input, int*) image identification, different animal samples on the same image share the same *image_id*.\n",
    "- **image :**(*input, array[1024*1024*3]*) image data of the sample.\n",
    "- **keypoints :** (*output, array[20*3]*) list of individual keypoints, which are lists of three values : [$x_{pos}$, $y_{pos}$, conf].\n",
    "- **bbox :** (*output, array[4]*) coordinates [$x$,$y$] for diagonal corners defining the bounding box of the animal.\n",
    "- **label :** (*output, int*) class id of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cow sheep horse cat dog\n",
    "labels = {'dog':1, 'cat':2, 'sheep':3, 'horse':4, 'cow':5}\n",
    "\n",
    "class AnimalPoseDataset(Dataset):\n",
    "    def __init__ (self, json_file, root_dir, transform=None):\n",
    "        self.keypoints_frame = json.load(open(json_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keypoints_frame[\"annotations\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        anno_dict = self.keypoints_frame\n",
    "        img_id = anno_dict[\"annotations\"][idx][\"image_id\"]\n",
    "        image_map = anno_dict[\"images\"]\n",
    "        annotations = anno_dict[\"annotations\"]\n",
    "\n",
    "        imagename = image_map[str(annotations[idx][\"image_id\"])]\n",
    "        bbox = torch.tensor(annotations[idx][\"bbox\"])\n",
    "        keypoints = annotations[idx][\"keypoints\"]\n",
    "        label = annotations[idx][\"category_id\"] \n",
    "        image_path = os.path.join(self.root_dir, imagename)\n",
    "        image = cv2.imread(image_path)\n",
    "        sample = {'image_id': img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def draw(self, sample):\n",
    "        image = sample['image']\n",
    "        bbox = sample['bbox']\n",
    "        xmin, ymin, xmax, ymax = bbox \n",
    "        image = draw_bbox(image, xmin, ymin, xmax, ymax, random_color())\n",
    "        image = draw_keypoint(image, sample['keypoints'])\n",
    "        return image\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "  return result\n",
    "\n",
    "class Rescale (object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__ (self, sample):\n",
    "        img_id, image, keypoints, bbox = sample['image_id'],sample['image'], sample['keypoints'], sample['bbox']\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        # scale the image\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        # scale the keypoints\n",
    "        scaled_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            new_x = int(kp[0] * new_w / w)\n",
    "            new_y = int(kp[1] * new_h / h)\n",
    "            scaled_keypoints.append([new_x, new_y, kp[2]])\n",
    "        # convert to tensor\n",
    "        scaled_keypoints = torch.tensor(scaled_keypoints)\n",
    "        # scale the bbox\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        xmin = int(xmin * new_w / w)\n",
    "        xmax = int(xmax * new_w / w)\n",
    "        ymin = int(ymin * new_h / h)\n",
    "        ymax = int(ymax * new_h / h)\n",
    "        # convert to tensor\n",
    "        bbox = torch.tensor([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        return {'image_id':img_id, 'image': img, 'keypoints': scaled_keypoints, 'bbox':bbox, 'label':sample['label']}\n",
    "        \n",
    "class SDA(object):\n",
    "    \n",
    "    def __init__(self, nb_bodyparts, tolerance=20):\n",
    "        # number of body parts to add to the image\n",
    "        self.nb_bodyparts = nb_bodyparts\n",
    "        self.bodypart_pool = []\n",
    "        self.tolerance=tolerance\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_id, image, keypoints, bbox, label = sample['image_id'], sample['image'], sample['keypoints'], sample['bbox'], sample['label']\n",
    "        image, keypoints, bodyparts = self.crop_bodypart(image, keypoints)\n",
    "        self.bodypart_pool.extend(bodyparts)\n",
    "        \n",
    "        # add the body parts to the image\n",
    "        for i in range(self.nb_bodyparts):\n",
    "            image = self.add_bodyparts(image)\n",
    "        \n",
    "        return {'image_id':img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':label}\n",
    "\n",
    "    def crop_bodypart(self, image, keypoints):\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        draw_keypoint(mask, keypoints)\n",
    "        # find the contours in the mask\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # crop the different body parts and store them \n",
    "        bodyparts = []\n",
    "        for i in range(len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            bodyparts.append(image[y-self.tolerance:y+h+self.tolerance, x-self.tolerance:x+w+self.tolerance])\n",
    "        # return the image with the body parts and the keypoints\n",
    "        return image, keypoints, bodyparts\n",
    "    \n",
    "    def add_bodyparts(self, image):        \n",
    "        # randomly select a body part\n",
    "        # check if the body part pool is empty\n",
    "        if len(self.bodypart_pool) == 0:\n",
    "            return image\n",
    "        bodypart = random.choice(self.bodypart_pool)\n",
    "        # randomly select an angle\n",
    "        #angle = random.randint(0, 360)        \n",
    "        # rotate the body part\n",
    "        #bodypart = rotate_image(bodypart, angle)\n",
    "        h,w,_ = bodypart.shape\n",
    "\n",
    "        # randomly select a position for the body part\n",
    "        x = random.randint(0, image.shape[1] - w)\n",
    "        y = random.randint(0, image.shape[0] - h)\n",
    "        \n",
    "        image[y:y+h, x:x+w] = cv2.addWeighted(image[y:y+h, x:x+w], 0, bodypart, 1, 0)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def show_bodyparts(self):     \n",
    "        for i in range(len(self.bodypart_pool)):\n",
    "            plt.imshow(self.bodypart_pool[i])\n",
    "            plt.show()\n",
    "#TODO: adapt SDA so it has a limited body part pool, if else it will consume too much memory\n",
    "dataset = AnimalPoseDataset(json_file='Dataset/keypoints.json', \n",
    "                            root_dir='Dataset/images/',\n",
    "                            transform=transforms.Compose([Rescale((640,640))]))\n",
    "                                                        \n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "sample = {'image_id':None, 'image': None, 'keypoints': None, 'bbox':None, 'label':None}\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    #print(i_batch, sample_batched['image'].size(), sample_batched['label'])\n",
    "    if i_batch == 3:\n",
    "        sample['image_id'] = sample_batched['image_id']\n",
    "        sample['image'] = sample_batched['image']\n",
    "        sample['keypoints'] = sample_batched['keypoints']\n",
    "        sample['bbox'] = sample_batched['bbox']\n",
    "        sample['label'] = sample_batched['label']\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **k** : kernel size\n",
    "- **s** : stride, determines the step size the convolution filter moves across the input image.\n",
    "- **p** : padding, used to control the spatial dimensions of the ouptut feature map by adding extra pixels around the input image or feature map before applying the convolution\n",
    "\n",
    "Special components:\n",
    "- **Split**: divides the input feature map into two or more separate feature maps along a specified axis(usually channel axis). This can be useful for processing parts of the feature map separately of feeding them into different parallel sub-nets\n",
    "- **Bottleneck**: design pattern often used to reduce the dimensionality of feature maps, followed by an expansion to the original dimensionality. This is usually achieved using a series of conv layers with varying kernels sizes and channel dimensions. Helps in reducing the models computational complexity while preserving relevant features\n",
    "- **Concat**: the concatenation op combines multiple feature maps along a specified axis. This is useful for merging information from different sources or resolutions withing the network, which can help improve the model's ability ot learn complex features and relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (100000.000000 --> 0.620792).  Saving model ...\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 0.620792 \tRemaining: 6116\n",
      "Validation loss decreased (3973.066406 --> 0.919886).  Saving model ...\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 0.919886 \tRemaining: 6115\n",
      "Validation loss decreased (3828.404541 --> 1.090806).  Saving model ...\n",
      "Epoch: 1 \tBatch: 3 \tLoss: 1.090806 \tRemaining: 6114\n",
      "Validation loss decreased (3281.671875 --> 1.206702).  Saving model ...\n",
      "Epoch: 1 \tBatch: 4 \tLoss: 1.206702 \tRemaining: 6113\n",
      "Epoch: 1 \tBatch: 5 \tLoss: 1.320293 \tRemaining: 6112\n",
      "Epoch: 1 \tBatch: 6 \tLoss: 1.411673 \tRemaining: 6111\n",
      "Epoch: 1 \tBatch: 7 \tLoss: 1.506589 \tRemaining: 6110\n",
      "Epoch: 1 \tBatch: 8 \tLoss: 1.591035 \tRemaining: 6109\n",
      "Epoch: 1 \tBatch: 9 \tLoss: 1.661139 \tRemaining: 6108\n",
      "Epoch: 1 \tBatch: 10 \tLoss: 1.746299 \tRemaining: 6107\n",
      "Epoch: 1 \tBatch: 11 \tLoss: 1.803783 \tRemaining: 6106\n",
      "Epoch: 1 \tBatch: 12 \tLoss: 1.927375 \tRemaining: 6105\n",
      "Epoch: 1 \tBatch: 13 \tLoss: 2.005238 \tRemaining: 6104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 321\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39m#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.00005, max_lr=0.002,step_size_up=10,mode='exp_range',cycle_momentum=False)\u001b[39;00m\n\u001b[0;32m    320\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m--> 321\u001b[0m train_net(net, dataloader, n_epochs, optimizer, criterion)\n",
      "Cell \u001b[1;32mIn[29], line 279\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(net, train_loader, n_epochs, optimizer, criterion)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m# calculate the batch loss\u001b[39;00m\n\u001b[0;32m    278\u001b[0m bbox \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 279\u001b[0m loss1, _ \u001b[39m=\u001b[39m criterion(output[\u001b[39m1\u001b[39;49m], bbox)\n\u001b[0;32m    280\u001b[0m loss2,_ \u001b[39m=\u001b[39m criterion(output[\u001b[39m3\u001b[39m], bbox)\n\u001b[0;32m    281\u001b[0m loss3,_ \u001b[39m=\u001b[39m criterion(output[\u001b[39m5\u001b[39m], bbox)\n",
      "Cell \u001b[1;32mIn[29], line 239\u001b[0m, in \u001b[0;36mbbox_regression_loss\u001b[1;34m(pred_bboxes, gt_bboxes)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pred_bboxes_abs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    237\u001b[0m     \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pred_bboxes_abs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]):\n\u001b[1;32m--> 239\u001b[0m         iou \u001b[39m=\u001b[39m calculate_iou(pred_bboxes_abs[sample, :, bbox], gt_bboxes[sample, :])\n\u001b[0;32m    240\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m iou\n\u001b[0;32m    241\u001b[0m         iou_loss[sample, bbox] \u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[1;32mIn[29], line 206\u001b[0m, in \u001b[0;36mcalculate_iou\u001b[1;34m(pred_bboxes, gt_bboxes)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_iou\u001b[39m(pred_bboxes, gt_bboxes):\n\u001b[0;32m    205\u001b[0m      \u001b[39m# Calculate intersection\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m     inter_xmin \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(pred_bboxes[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m], gt_bboxes[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m])\n\u001b[0;32m    207\u001b[0m     inter_ymin \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(pred_bboxes[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m], gt_bboxes[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m    208\u001b[0m     inter_xmax \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(pred_bboxes[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m2\u001b[39m], gt_bboxes[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m2\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Detail block modules\n",
    "# Conv\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, k, s, p, c_in, c_out):\n",
    "        super().__init__()\n",
    "        # 2d conv layer\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p)\n",
    "        # batch normalization\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        # SiLU activation\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, shortcut, h, w , c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in//2)\n",
    "        self.conv1 = Conv(k=3, s=1, p=1, c_in=c_in//2, c_out=c_in)\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shortcut:\n",
    "            xp = self.conv1(self.conv(x))\n",
    "            x = x + xp\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            x = self.conv1(x)        \n",
    "        return x\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=1, s=1, p=0, c_in=c_in,c_out=c_in)    \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=32, padding=2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.maxpool(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        xtot = x1 + x2 + x3 + x4\n",
    "        #x = torch.cat((x4, xtot), dim=1)\n",
    "        x = self.conv(xtot)\n",
    "        return x\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, shortcut, c_in, c_out, h, w, n):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(k=1, s=1, p=0, c_in=c_in, c_out=c_out)\n",
    "        self.conv2 = Conv(k=1, s=1, p=0, c_in=int(0.5*(n+2)*c_out), c_out=c_out)\n",
    "        self.bottleneck = Bottleneck(shortcut=shortcut, h=h, w=w, c_in=c_out//2)\n",
    "        self.n = n\n",
    "        self.cout = c_out\n",
    "        self.cin = c_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # split the input into half and the other half channels\n",
    "        split_x = torch.split(x, x.size(1)//2, dim=1)        \n",
    "        # current bottleneck\n",
    "        bn = split_x[0]\n",
    "        # list of bottlenecks to be stacked\n",
    "        bn_x = []\n",
    "        bn_x.append(bn)        \n",
    "        # iterate through the number of bottlenecks to be stacked\n",
    "        for i in range(self.n):\n",
    "            # apply the bottleneck to the first half channels and store them in bn_x\n",
    "            bn = self.bottleneck(bn)\n",
    "            bn_x.append(bn)\n",
    "               \n",
    "        # concatenate the first half channels with the second half channels\n",
    "        bn_x.append(split_x[1])\n",
    "        # concatenate the bottlenecks\n",
    "        x = torch.cat(bn_x, dim=1)\n",
    "        x = self.conv2(x)                \n",
    "        return x\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, reg_max, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in)\n",
    "        self.conv2d_bbox = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=4*reg_max)\n",
    "        self.bn1 = nn.BatchNorm2d(4*reg_max)\n",
    "        self.conv2d_cls = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=num_classes)\n",
    "        self.bn2 = nn.BatchNorm2d(num_classes)\n",
    "        self.linear = nn.Linear(in_features=6400, out_features=4)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv(x)\n",
    "        x_cls = self.conv2d_cls(x)\n",
    "        x_cls = self.bn2(x_cls)\n",
    "        x_bbox = self.conv2d_bbox(x)\n",
    "        x_bbox = self.bn1(x_bbox)\n",
    "        #x_bbox = self.calculate_bbox(x_bbox)\n",
    "        return x_cls, x_bbox\n",
    "\n",
    "    def calculate_bbox(self, x):\n",
    "        x_pred = torch.sigmoid(x[:,0,:, :])\n",
    "        y_pred = torch.sigmoid(x[:,1,:, :])\n",
    "        w_pred = torch.exp(x[:,2,:, :])\n",
    "        h_pred = torch.exp(x[:,3,:, :])\n",
    "        bbox = torch.cat([x_pred, y_pred, w_pred, h_pred], dim=1)\n",
    "        return bbox\n",
    "\n",
    "# Main Network architecture\n",
    "class BBoxNet(nn.Module):\n",
    "    def __init__(self, w, r, d):\n",
    "        super(BBoxNet, self).__init__()\n",
    "    # backbone network modules\n",
    "        self.conv_0_p1 = Conv(k=3, s=2, p=1, c_in=3, c_out=int(64*w))\n",
    "        self.conv_1_p2 = Conv(k=3, s=2, p=1, c_in=int(64*w), c_out=int(128*w))\n",
    "        self.c2f_2 = C2f(shortcut=True, h=160, w=160, n=int(3*d), c_in=int(128*w), c_out=int(128*w))\n",
    "        self.conv_3_p3 = Conv(k=3, s=2, p=1, c_in=int(128*w), c_out=int(256*w))\n",
    "        self.c2f_4 = C2f(shortcut=True, h=80, w=80, n=int(6*d), c_in=int(256*w), c_out=int(256*w))\n",
    "        self.conv_5_p4 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(512*w))\n",
    "        self.c2f_6 = C2f(shortcut=True, h=40, w=40, n=int(6*d), c_in=int(512*w), c_out=int(512*w))\n",
    "        self.conv_7_p5 = Conv(k=3, s=2, p=1, c_in=int(512*w), c_out=int(512*w*r))\n",
    "        self.c2f_8 = C2f(shortcut=True, h=20, w=20, n=int(3*d), c_in=int(512*w*r), c_out=int(512*r*w))\n",
    "        self.sppf_9 = SPPF(c_in=int(512*w*r))\n",
    "\n",
    "    # head network modules\n",
    "        self.upsample_10 = nn.Upsample(size=(40,40), mode='bilinear', align_corners=False)\n",
    "        self.concat_11 = torch.cat\n",
    "        self.c2f_12 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.upsample_resolution_13a = nn.Upsample(size=(80,80), mode='bilinear', align_corners=False)\n",
    "        self.upsample_channels_13b = nn.Conv2d(in_channels=int(512*w), out_channels=int(256*w), kernel_size=1)\n",
    "        self.concat_14 = torch.cat\n",
    "        self.c2f_15 = C2f(shortcut=False, c_in=int(512*w), c_out=int(256*w), h=80, w=80, n=int(3*d))\n",
    "        self.conv_16_p3 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(256*w))\n",
    "        self.concat_17 = torch.cat\n",
    "        # ISSUE HERE, THE ARCHITECTURE OUTPUT CHANNEL SIZE IS PROBABLY WRONG, AS THE CONCATENATION DOES NOT INCREASE THE CHANNEL SIZE\n",
    "        #self.c2f_18 = C2f(shortcut=False, c_in=int(512*w), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.c2f_18 = C2f(shortcut=False, c_in=192, c_out=192, h=40, w=40, n=int(3*d))\n",
    "        self.conv_19 = Conv(k=3, s=2, p=1, c_in=192, c_out=192)\n",
    "        self.concat_20 = torch.cat\n",
    "        #self.c2f_21 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "        self.c2f_21 = C2f(shortcut=False, c_in=448, c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "    \n",
    "    # output layers\n",
    "        self.detect1 = Detect(num_classes=6, reg_max=1, c_in=int(256*w))\n",
    "        self.detect2 = Detect(num_classes=6, reg_max=1, c_in=192)\n",
    "        self.detect3 = Detect(num_classes=6, reg_max=1, c_in=int(512*w))\n",
    "\n",
    "    def forward(self,x):\n",
    "    # backbone pass\n",
    "        x = self.conv_0_p1(x)\n",
    "        x = self.conv_1_p2(x)\n",
    "        x = self.c2f_2(x)\n",
    "        x = self.conv_3_p3(x)\n",
    "        x = self.c2f_4(x)\n",
    "        # save for concat later\n",
    "        x_4 = x\n",
    "        \n",
    "        x = self.conv_5_p4(x)\n",
    "        x = self.c2f_6(x)\n",
    "        \n",
    "        # save for concat later\n",
    "        x_6 = x\n",
    "        x = self.conv_7_p5(x)\n",
    "        x = self.c2f_8(x)\n",
    "        x = self.sppf_9(x)\n",
    "        x_9 = x\n",
    "\n",
    "    # head pass\n",
    "        # first brancH\n",
    "        x = self.upsample_10(x)\n",
    "        x = self.concat_11((x, x_6), dim=1)\n",
    "        x = self.c2f_12(x)  \n",
    "        x_12 = x\n",
    "        x = self.upsample_resolution_13a(x)\n",
    "        x = self.upsample_channels_13b(x)\n",
    "        x = self.concat_14((x, x_4), dim=1)\n",
    "        x = self.c2f_15(x) \n",
    "        x_detect1 = x\n",
    "        \n",
    "    # second branch\n",
    "        x = self.conv_16_p3(x)\n",
    "        # CHECK CHANNEL ISSUE HEREISSUE HERE\n",
    "        x = self.concat_17((x_12, x), dim=1)\n",
    "        x = self.c2f_18(x)\n",
    "        x_detect2 = x\n",
    "        x = self.conv_19(x)\n",
    "        # ISSUE PROPAGATES HERE ALSO\n",
    "        x = self.concat_20((x, x_9), dim=1)\n",
    "        x = self.c2f_21(x)\n",
    "        x_detect3 = x\n",
    "    \n",
    "    # output layers\n",
    "        x_cls1, x_bbox1 = self.detect1(x_detect1)\n",
    "        x_cls2, x_bbox2 = self.detect2(x_detect2)\n",
    "        x_cls3, x_bbox3 = self.detect3(x_detect3)        \n",
    "\n",
    "        return [x_cls1, x_bbox1, x_cls2, x_bbox2, x_cls3, x_bbox3]\n",
    "\n",
    "def calculate_iou(pred_bboxes, gt_bboxes):\n",
    "     # Calculate intersection\n",
    "    inter_xmin = torch.max(pred_bboxes[..., 0], gt_bboxes[..., 0])\n",
    "    inter_ymin = torch.max(pred_bboxes[..., 1], gt_bboxes[..., 1])\n",
    "    inter_xmax = torch.min(pred_bboxes[..., 2], gt_bboxes[..., 2])\n",
    "    inter_ymax = torch.min(pred_bboxes[..., 3], gt_bboxes[..., 3])\n",
    "\n",
    "    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
    "    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # Calculate union\n",
    "    pred_area = (pred_bboxes[..., 2] - pred_bboxes[..., 0]) * (pred_bboxes[..., 3] - pred_bboxes[..., 1])\n",
    "    gt_area = (gt_bboxes[..., 2] - gt_bboxes[..., 0]) * (gt_bboxes[..., 3] - gt_bboxes[..., 1])\n",
    "    union_area = pred_area + gt_area - inter_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "    \n",
    "def bbox_regression_loss(pred_bboxes, gt_bboxes):\n",
    "    # Reshape the predicted bounding boxes to (batch, 4, -1)\n",
    "    pred_bboxes = pred_bboxes.view(pred_bboxes.size(0), 4, -1)\n",
    "    # Convert the predicted bounding boxes to absolute coordinates\n",
    "    pred_bboxes_xy = torch.sigmoid(pred_bboxes[:, :2, :])\n",
    "    pred_bboxes_wh = torch.exp(pred_bboxes[:, 2:, :])\n",
    "    # Combine the x, y, width, and height to create the final predicted bounding boxes\n",
    "    pred_bboxes_abs = torch.cat((pred_bboxes_xy, pred_bboxes_wh), dim=1)\n",
    "    iou_loss = torch.zeros((pred_bboxes_abs.shape[0], pred_bboxes_abs.shape[2]))\n",
    "    id_list = torch.zeros((pred_bboxes_abs.shape[0], pred_bboxes_abs.shape[1]))\n",
    "\n",
    "\n",
    "    for sample in range(pred_bboxes_abs.shape[0]):\n",
    "        for bbox in range(pred_bboxes_abs.shape[2]):\n",
    "            \n",
    "            iou = calculate_iou(pred_bboxes_abs[sample, :, bbox], gt_bboxes[sample, :])\n",
    "            loss = 1 - iou\n",
    "            iou_loss[sample, bbox] = loss\n",
    "\n",
    "        # get the index of the predicted bounding box with the highest IoU\n",
    "        max_iou, max_iou_idx = torch.max(iou_loss[sample], dim=0)\n",
    "        # if the IoU is greater than 0.5, then the predicted bounding box is a true positive\n",
    "        if max_iou > 0.5:\n",
    "            id_list[sample, :] = pred_bboxes_abs[sample, :, max_iou_idx]\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = torch.sum(torch.abs(id_list - gt_bboxes))\n",
    "    return loss, id_list\n",
    "\n",
    "    \n",
    "# define the training function\n",
    "def train_net(net, train_loader, n_epochs, optimizer, criterion):\n",
    "    # loop over the number of epochs\n",
    "    best_loss = 100000\n",
    "    for epoch in range(n_epochs):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # set the model to training mode\n",
    "        net.train()\n",
    "        for i_batch, data in enumerate(dataloader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data['image'] = data['image'].cuda()\n",
    "                data['bbox'] = data['bbox'].cuda()\n",
    "                data['label'] = data['label'].cuda()\n",
    "                data['image_id'] = data['image_id'].cuda()\n",
    "                \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = net(data['image'].permute(0,3,1,2).float())\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            bbox = torch.stack([t for t in data['bbox']], dim=0)\n",
    "            loss1, _ = criterion(output[1], bbox)\n",
    "            loss2,_ = criterion(output[3], bbox)\n",
    "            loss3,_ = criterion(output[5], bbox)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "\n",
    "            train_loss += loss.item()*data['image'].size(0)\n",
    "            # save the model if validation loss has decreased\n",
    "            if loss.item() < best_loss:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_loss, loss.item()))\n",
    "                torch.save(net.state_dict(), 'model.pt')\n",
    "                best_loss = loss.item()\n",
    "    \n",
    "            # save the three feature maps with different name        \n",
    "            #if i_batch%10 == 0:\n",
    "            #    plt.imsave(\"Output/feature_map1_\"+str(i_batch)+\".jpg\", output[0][0,0,:,:].detach().cpu().numpy())\n",
    "            #    plt.imsave(\"Output/feature_map2_\"+str(i_batch)+\".jpg\", output[1][0,0,:,:].detach().cpu().numpy())\n",
    "            #    plt.imsave(\"Output/feature_map3_\"+str(i_batch)+\".jpg\", output[2][0,0,:,:].detach().cpu().numpy())\n",
    "\n",
    "            # print train loss in percentage, and remaining batches in epoch\n",
    "            print('Epoch: {} \\tBatch: {} \\tLoss: {:.6f} \\tRemaining: {}'.format(epoch+1, i_batch+1, loss.item(), len(train_loader)-i_batch-1))\n",
    "            \n",
    "\n",
    "        # print training statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "\n",
    "        # step the scheduler\n",
    "        #scheduler.step()\n",
    "net = BBoxNet(w=0.25, r=2, d=0.34)\n",
    "net = net.to(device)\n",
    "criterion = bbox_regression_loss\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.00005, max_lr=0.002,step_size_up=10,mode='exp_range',cycle_momentum=False)\n",
    "n_epochs = 10\n",
    "train_net(net, dataloader, n_epochs, optimizer, criterion)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenPose Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 150\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    147\u001b[0m                 \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m    148\u001b[0m                     \u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, num_epochs, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, total_step, loss\u001b[39m.\u001b[39mitem(), accuracy))\n\u001b[1;32m--> 150\u001b[0m train(model, dataloader, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m    130\u001b[0m images \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    131\u001b[0m labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 132\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m    133\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    135\u001b[0m \u001b[39m# Backprop and perform Adam optimisation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[10], line 112\u001b[0m, in \u001b[0;36mIndiaNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstage_vgg(x)\n\u001b[0;32m    113\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage_phi(x)\n\u001b[0;32m    114\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage_rho(x)\n",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m, in \u001b[0;36mIndiaNet.stage_vgg\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstage_vgg\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 73\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1_1(x)))\n\u001b[0;32m     74\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2_4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2_3(x)))\n\u001b[0;32m     75\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1_5(x)\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        return x\n",
    "\n",
    "class IndiaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IndiaNet, self).__init__()\n",
    "    # vgg 19 first 10 layers\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_3 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn2_4 = nn.BatchNorm2d(64)\n",
    "        self.pool1_5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3_6 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.bn3_7 = nn.BatchNorm2d(128)\n",
    "        self.conv4_8 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.bn4_9 = nn.BatchNorm2d(128)\n",
    "        self.pool2_10 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5_11 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.bn5_12 = nn.BatchNorm2d(256)\n",
    "        self.conv6_13 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn6_14 = nn.BatchNorm2d(256)\n",
    "        self.conv7_15 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn7_16 = nn.BatchNorm2d(256)\n",
    "        self.conv8_17 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.bn8_18 = nn.BatchNorm2d(256)\n",
    "        self.pool3_19 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv9_20 = nn.Conv2d(256, 512, 3, 1, 1)\n",
    "        self.bn9_21 = nn.BatchNorm2d(512)\n",
    "        self.conv10_22 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        self.bn10_23 = nn.BatchNorm2d(512)\n",
    "    \n",
    "    # phi stage\n",
    "        self.convb1_24 = ConvBlock(512, 512)\n",
    "        self.convb2_25 = ConvBlock(512, 512)\n",
    "        self.convb3_26 = ConvBlock(512, 512)\n",
    "        self.convb4_27 = ConvBlock(512, 512)\n",
    "        self.convb5_28 = ConvBlock(512, 512)\n",
    "        self.conv11_29 = nn.Conv2d(512, 512, 1, 1, 1)\n",
    "        self.bn11_30 = nn.BatchNorm2d(512)\n",
    "        self.conv12_31 = nn.Conv2d(512, 512, 1, 1, 1)\n",
    "        self.bn12_32 = nn.BatchNorm2d(512)\n",
    "\n",
    "    # rho stage\n",
    "        self.convb6_33 = ConvBlock(512, 512)\n",
    "        self.convb7_34 = ConvBlock(512, 512)\n",
    "        self.convb8_35 = ConvBlock(512, 512)\n",
    "        self.convb9_36 = ConvBlock(512, 512)\n",
    "        self.convb10_37 = ConvBlock(512, 512)\n",
    "        self.conv13_38 = nn.Conv2d(512, 512, 1, 1, 1)\n",
    "        self.bn13_39 = nn.BatchNorm2d(512)\n",
    "        self.conv14_40 = nn.Conv2d(512, 512, 1, 1, 1)\n",
    "        self.bn14_41 = nn.BatchNorm2d(512) \n",
    "\n",
    "    # vgg stage\n",
    "    def stage_vgg(self, x):\n",
    "        x = F.relu(self.bn1_2(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn2_4(self.conv2_3(x)))\n",
    "        x = self.pool1_5(x)\n",
    "        x = F.relu(self.bn3_7(self.conv3_6(x)))\n",
    "        x = F.relu(self.bn4_9(self.conv4_8(x)))\n",
    "        x = self.pool2_10(x)\n",
    "        x = F.relu(self.bn5_12(self.conv5_11(x)))\n",
    "        x = F.relu(self.bn6_14(self.conv6_13(x)))\n",
    "        x = F.relu(self.bn7_16(self.conv7_15(x)))\n",
    "        x = F.relu(self.bn8_18(self.conv8_17(x)))\n",
    "        x = self.pool3_19(x)\n",
    "        x = F.relu(self.bn9_21(self.conv9_20(x)))\n",
    "        x = F.relu(self.bn10_23(self.conv10_22(x)))\n",
    "        return x\n",
    "    \n",
    "    # phi stage\n",
    "    def stage_phi(self, x):\n",
    "        x = self.convb1_24(x)\n",
    "        x = self.convb2_25(x)\n",
    "        x = self.convb3_26(x)\n",
    "        x = self.convb4_27(x)\n",
    "        x = self.convb5_28(x)\n",
    "        x = F.relu(self.bn11_30(self.conv11_29(x)))\n",
    "        x = F.relu(self.bn12_32(self.conv12_31(x)))\n",
    "        return x\n",
    "    \n",
    "    # rho stage\n",
    "    def stage_rho(self, x):\n",
    "        x = self.convb6_33(x)\n",
    "        x = self.convb7_34(x)\n",
    "        x = self.convb8_35(x)\n",
    "        x = self.convb9_36(x)\n",
    "        x = self.convb10_37(x)\n",
    "        x = F.relu(self.bn13_39(self.conv13_38(x)))\n",
    "        x = F.relu(self.bn14_41(self.conv14_40(x)))\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stage_vgg(x)\n",
    "        x = self.stage_phi(x)\n",
    "        x = self.stage_rho(x)\n",
    "        return x    \n",
    "    \n",
    "model = IndiaNet()\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, num_epochs):\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Run the forward pass\n",
    "            images = data['image']\n",
    "            labels = data['label']\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backprop and perform Adam optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {}%' \n",
    "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item(), accuracy))\n",
    "                \n",
    "train(model, dataloader, num_epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  Dataset.visualize_keypoint import *\n",
    "#from src.dataset import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the following features:\n",
    "- **image_id :** (*input, int*) image identification, different animal samples on the same image share the same *image_id*.\n",
    "- **image :**(*input, array[1024*1024*3]*) image data of the sample.\n",
    "- **keypoints :** (*output, array[20*3]*) list of individual keypoints, which are lists of three values : [$x_{pos}$, $y_{pos}$, conf].\n",
    "- **bbox :** (*output, array[4]*) coordinates [$x$,$y$] for diagonal corners defining the bounding box of the animal.\n",
    "- **label :** (*output, int*) class id of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cow sheep horse cat dog\n",
    "labels = ['dog', 'cat', 'sheep', 'horse', 'cow']\n",
    "\n",
    "class AnimalPoseDataset(Dataset):\n",
    "    def __init__ (self, json_file, root_dir, transform=None):\n",
    "        self.keypoints_frame = json.load(open(json_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keypoints_frame[\"annotations\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        anno_dict = self.keypoints_frame\n",
    "        img_id = anno_dict[\"annotations\"][idx][\"image_id\"]\n",
    "        image_map = anno_dict[\"images\"]\n",
    "        annotations = anno_dict[\"annotations\"]\n",
    "\n",
    "        imagename = image_map[str(annotations[idx][\"image_id\"])]\n",
    "        bbox = annotations[idx][\"bbox\"]\n",
    "        keypoints = annotations[idx][\"keypoints\"]\n",
    "        label = labels[annotations[idx][\"category_id\"]-1]\n",
    "        image_path = os.path.join(self.root_dir, imagename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        sample = {'image_id': img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def draw(self, sample):\n",
    "        image = sample['image']\n",
    "        bbox = sample['bbox']\n",
    "        xmin, ymin, xmax, ymax = bbox \n",
    "        image = draw_bbox(image, xmin, ymin, xmax, ymax, random_color())\n",
    "        image = draw_keypoint(image, sample['keypoints'])\n",
    "        return image\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "  return result\n",
    "\n",
    "class Rescale (object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__ (self, sample):\n",
    "        img_id, image, keypoints, bbox = sample['image_id'],sample['image'], sample['keypoints'], sample['bbox']\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        # scale the image\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        # scale the keypoints\n",
    "        scaled_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            new_x = int(kp[0] * new_w / w)\n",
    "            new_y = int(kp[1] * new_h / h)\n",
    "            scaled_keypoints.append([new_x, new_y, kp[2]])\n",
    "        # scale the bbox\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        xmin = int(xmin * new_w / w)\n",
    "        xmax = int(xmax * new_w / w)\n",
    "        ymin = int(ymin * new_h / h)\n",
    "        ymax = int(ymax * new_h / h)\n",
    "        bbox = [xmin, ymin, xmax, ymax]\n",
    "        \n",
    "\n",
    "        return {'image_id':img_id, 'image': img, 'keypoints': scaled_keypoints, 'bbox':bbox, 'label':sample['label']}\n",
    "        \n",
    "class SDA(object):\n",
    "    \n",
    "    def __init__(self, nb_bodyparts, tolerance=20):\n",
    "        # number of body parts to add to the image\n",
    "        self.nb_bodyparts = nb_bodyparts\n",
    "        self.bodypart_pool = []\n",
    "        self.tolerance=tolerance\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_id, image, keypoints, bbox = sample['image_id'], sample['image'], sample['keypoints'], sample['bbox']\n",
    "        image, keypoints, bodyparts = self.crop_bodypart(image, keypoints)\n",
    "        self.bodypart_pool.extend(bodyparts)\n",
    "        \n",
    "        # add the body parts to the image\n",
    "        for i in range(self.nb_bodyparts):\n",
    "            image = self.add_bodyparts(image)\n",
    "            \n",
    "        return {'image_id':img_id, 'image': image, 'keypoints': keypoints, 'bbox':bbox, 'label':sample['label']}\n",
    "\n",
    "    def crop_bodypart(self, image, keypoints):\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        draw_keypoint(mask, keypoints)\n",
    "        # find the contours in the mask\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # crop the different body parts and store them \n",
    "        bodyparts = []\n",
    "        for i in range(len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            bodyparts.append(image[y-self.tolerance:y+h+self.tolerance, x-self.tolerance:x+w+self.tolerance])\n",
    "        # return the image with the body parts and the keypoints\n",
    "        return image, keypoints, bodyparts\n",
    "    \n",
    "    def add_bodyparts(self, image):        \n",
    "        # randomly select a body part\n",
    "        bodypart = random.choice(self.bodypart_pool)\n",
    "        # randomly select an angle\n",
    "        #angle = random.randint(0, 360)        \n",
    "        # rotate the body part\n",
    "        #bodypart = rotate_image(bodypart, angle)\n",
    "        h,w,_ = bodypart.shape\n",
    "\n",
    "        # randomly select a position for the body part\n",
    "        x = random.randint(0, image.shape[1] - w)\n",
    "        y = random.randint(0, image.shape[0] - h)\n",
    "        \n",
    "        image[y:y+h, x:x+w] = cv2.addWeighted(image[y:y+h, x:x+w], 0, bodypart, 1, 0)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def show_bodyparts(self):     \n",
    "        for i in range(len(self.bodypart_pool)):\n",
    "            plt.imshow(self.bodypart_pool[i])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 640, 640, 3]) ['dog', 'cat', 'sheep', 'cat', 'cow', 'sheep', 'cat', 'dog', 'dog', 'cow']\n",
      "1 torch.Size([10, 640, 640, 3]) ['cat', 'dog', 'cat', 'horse', 'horse', 'cat', 'cat', 'dog', 'cow', 'dog']\n",
      "2 torch.Size([10, 640, 640, 3]) ['dog', 'dog', 'dog', 'cow', 'horse', 'sheep', 'dog', 'sheep', 'dog', 'horse']\n",
      "3 torch.Size([10, 640, 640, 3]) ['dog', 'sheep', 'dog', 'sheep', 'cow', 'cow', 'cow', 'cow', 'dog', 'sheep']\n",
      "tensor([4090, 3202, 3278,   73, 1606, 3958, 1192,  530,  176, 3330])\n",
      "keypoints:  [[tensor([136, 423,   0, 433,  71,   0,   0, 116, 388,   0]), tensor([121, 187,   0, 261, 443,   0,   0, 389, 199,   0]), tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0])], [tensor([ 74, 384, 481, 424,   0, 535,   0, 240, 247,   0]), tensor([115, 194, 273, 261,   0,  80,   0, 198, 207,   0]), tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0])], [tensor([ 74, 417, 544, 430,  48, 601, 186,  93, 347,   0]), tensor([160, 245, 358, 264, 488, 124, 424, 433, 286,   0]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0])], [tensor([206,   0, 499, 433,  92,   0,   0, 139, 415,  81]), tensor([ 93,   0,  90, 259, 421,   0,   0, 369, 147, 517]), tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1])], [tensor([ 57, 369, 454, 423,   0, 492,   0, 232, 205,  78]), tensor([ 96, 180, 141, 259,   0,  54,   0, 173, 165, 344]), tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1])], [tensor([275,   0,   0,   0, 209,   0,   0,   0, 361, 419]), tensor([420,   0,   0,   0, 460,   0,   0,   0, 408, 551]), tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1])], [tensor([132,   0,   0,   0,   0, 416, 154,   0, 213,   0]), tensor([416,   0,   0,   0,   0, 326, 452,   0, 394,   0]), tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0])], [tensor([408,   0,   0,   0,   0,   0,   0, 238, 490,   0]), tensor([341,   0,   0,   0,   0,   0,   0, 346, 317,   0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0])], [tensor([  0,   0,   0,   0,   0, 110,  99, 256,   0,   0]), tensor([  0,   0,   0,   0,   0, 339, 448, 309,   0,   0]), tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0])], [tensor([278, 439,   0, 431, 207, 375, 154,   0, 328, 381]), tensor([539, 390,   0, 281, 530, 464, 477,   0, 436, 599]), tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1])], [tensor([171,   0,   0, 424,   0, 405, 134,   0, 226,   0]), tensor([526,   0,   0, 281,   0, 476, 484,   0, 428,   0]), tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0])], [tensor([491, 614,   0,   0,   0,   0,  97,   0, 505,   0]), tensor([456, 491,   0,   0,   0,   0, 477,   0, 343,   0]), tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0])], [tensor([329,   0,   0,   0,   0,  85,  89, 377,   0,   0]), tensor([433,   0,   0,   0,   0, 444, 477, 404,   0,   0]), tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0])], [tensor([273,   0,   0,   0,   0, 360,   0,   0, 276, 330]), tensor([608,   0,   0,   0,   0, 547,   0,   0, 504, 638]), tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1])], [tensor([171,   0,   0,   0,   0, 405, 156,   0, 182,   0]), tensor([586,   0,   0,   0,   0, 550, 503,   0, 526,   0]), tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0])], [tensor([489,   0,   0,   0,   0, 110,   0, 198, 509,   0]), tensor([522,   0,   0,   0,   0, 563,   0, 451, 389,   0]), tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0])], [tensor([322,   0,   0,   0,   0,  74,   0,   0,   0,   0]), tensor([484,   0,   0,   0,   0, 569,   0,   0,   0,   0]), tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])], [tensor([141, 381,   0, 428, 134, 524, 165, 130, 269,   0]), tensor([268, 264,   0, 270, 476, 179, 433, 418, 348,   0]), tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0])], [tensor([266, 320,   0, 419, 130, 358, 156, 129,   0, 344]), tensor([189, 172,   0, 263, 394, 105, 395, 183,   0, 363]), tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1])], [tensor([398,   0,   0,   0,   0,  57,  81, 257,   0,   0]), tensor([202,   0,   0,   0,   0, 140, 414, 361,   0,   0]), tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])]]\n",
      "bbox : [tensor([ 30, 353,   1, 392,  42,  32,  74,  40,  94,   1]), tensor([ 46,  51,  54, 257, 366,  25, 382, 156, 125, 163]), tensor([605, 640, 559, 436, 253, 612, 190, 454, 530, 638]), tensor([631, 559, 482, 281, 558, 595, 518, 440, 540, 640])]\n",
      "label : ['dog', 'sheep', 'dog', 'sheep', 'cow', 'cow', 'cow', 'cow', 'dog', 'sheep']\n",
      "torch.Size([10, 640, 640, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = AnimalPoseDataset(json_file='Dataset/keypoints.json', \n",
    "                            root_dir='Dataset/images/',\n",
    "                            transform=transforms.Compose([\n",
    "                                                        Rescale((640,640)),\n",
    "                                                        SDA(nb_bodyparts=3, tolerance=10)]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "sample = {'image_id':None, 'image': None, 'keypoints': None, 'bbox':None, 'label':None}\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(), sample_batched['label'])\n",
    "\n",
    "    if i_batch == 3:\n",
    "        sample['image_id'] = sample_batched['image_id']\n",
    "        sample['image'] = sample_batched['image']\n",
    "        sample['keypoints'] = sample_batched['keypoints']\n",
    "        sample['bbox'] = sample_batched['bbox']\n",
    "        sample['label'] = sample_batched['label']\n",
    "        break\n",
    "\n",
    "\n",
    "print(sample['image_id'])\n",
    "\n",
    "print(\"keypoints: \", sample['keypoints'])\n",
    "\n",
    "print(\"bbox :\", sample['bbox'])\n",
    "\n",
    "print(\"label :\",sample['label'])\n",
    "\n",
    "print(sample['image'].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **k** : kernel size\n",
    "- **s** : stride, determines the step size the convolution filter moves across the input image.\n",
    "- **p** : padding, used to control the spatial dimensions of the ouptut feature map by adding extra pixels around the input image or feature map before applying the convolution\n",
    "\n",
    "Special components:\n",
    "- **Split**: divides the input feature map into two or more separate feature maps along a specified axis(usually channel axis). This can be useful for processing parts of the feature map separately of feeding them into different parallel sub-nets\n",
    "- **Bottleneck**: design pattern often used to reduce the dimensionality of feature maps, followed by an expansion to the original dimensionality. This is usually achieved using a series of conv layers with varying kernels sizes and channel dimensions. Helps in reducing the models computational complexity while preserving relevant features\n",
    "- **Concat**: the concatenation op combines multiple feature maps along a specified axis. This is useful for merging information from different sources or resolutions withing the network, which can help improve the model's ability ot learn complex features and relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]])\n",
      "(tensor([[[0, 1],\n",
      "         [2, 3]]]), tensor([[[4, 5],\n",
      "         [6, 7]]]), tensor([[[ 8,  9],\n",
      "         [10, 11]]]))\n",
      "torch.Size([1, 2, 2])\n",
      "3\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3,2,2)\n",
    "print(x)\n",
    "x1 = torch.split(x, 1, dim=0)\n",
    "print(x1)\n",
    "print(x1[0].size())\n",
    "print(x.size(0))\n",
    "\n",
    "xtot = torch.cat(x1, dim=0)\n",
    "print(xtot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 6400])\n",
      "torch.Size([10, 4, 6400])\n",
      "10 4 6400\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [10, 6400] but got: [10, 4096].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 297\u001b[0m\n\u001b[0;32m    295\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[39m# train the network\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m train_net(net, dataloader, n_epochs, optimizer, scheduler, criterion)\n",
      "Cell \u001b[1;32mIn[143], line 257\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(net, train_loader, n_epochs, optimizer, scheduler, criterion)\u001b[0m\n\u001b[0;32m    251\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    252\u001b[0m \u001b[39m# show the image\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m#plt.imshow(data['image'][0])\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m#plt.show()\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[39m# forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m output \u001b[39m=\u001b[39m net(data[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mfloat())\n\u001b[0;32m    259\u001b[0m \u001b[39m# calculate the batch loss\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39mprint\u001b[39m(output[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[143], line 215\u001b[0m, in \u001b[0;36mBBoxNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m     x_detect3 \u001b[39m=\u001b[39m x\n\u001b[0;32m    214\u001b[0m \u001b[39m# output layers\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     x_cls1, x_bbox1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect1(x_detect1)\n\u001b[0;32m    216\u001b[0m     \u001b[39m#x_cls2, x_bbox2 = self.detect2(x_detect2)\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m#x_cls3, x_bbox3 = self.detect3(x_detect3)        \u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39mprint\u001b[39m(x_cls1\u001b[39m.\u001b[39mshape, x_bbox1\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[143], line 114\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mprint\u001b[39m(x_bbox\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    113\u001b[0m \u001b[39mprint\u001b[39m(x_bbox\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), x_bbox\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), x_bbox\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m))\n\u001b[1;32m--> 114\u001b[0m val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x_bbox)\n\u001b[0;32m    115\u001b[0m \u001b[39mprint\u001b[39m(val\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    116\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mflag\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [10, 6400] but got: [10, 4096]."
     ]
    }
   ],
   "source": [
    "### Detail block modules\n",
    "# Conv\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, k, s, p, c_in, c_out):\n",
    "        super().__init__()\n",
    "        # 2d conv layer\n",
    "        self.conv = nn.Conv2d(c_in, c_out, k, s, p)\n",
    "        # batch normalization\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        # SiLU activation\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, shortcut, h, w , c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in//2)\n",
    "        self.conv1 = Conv(k=3, s=1, p=1, c_in=c_in//2, c_out=c_in)\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shortcut:\n",
    "            xp = self.conv1(self.conv(x))\n",
    "            x = x + xp\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            x = self.conv1(x)        \n",
    "        return x\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=1, s=1, p=0, c_in=c_in,c_out=c_in)    \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=32, padding=2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.maxpool(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        xtot = x1 + x2 + x3 + x4\n",
    "        #x = torch.cat((x4, xtot), dim=1)\n",
    "        x = self.conv(xtot)\n",
    "        return x\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, shortcut, c_in, c_out, h, w, n):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(k=1, s=1, p=0, c_in=c_in, c_out=c_out)\n",
    "        self.conv2 = Conv(k=1, s=1, p=0, c_in=int(0.5*(n+2)*c_out), c_out=c_out)\n",
    "        self.bottleneck = Bottleneck(shortcut=shortcut, h=h, w=w, c_in=c_out//2)\n",
    "        self.n = n\n",
    "        self.cout = c_out\n",
    "        self.cin = c_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # split the input into half and the other half channels\n",
    "        split_x = torch.split(x, x.size(1)//2, dim=1)        \n",
    "        # current bottleneck\n",
    "        bn = split_x[0]\n",
    "        # list of bottlenecks to be stacked\n",
    "        bn_x = []\n",
    "        bn_x.append(bn)        \n",
    "        # iterate through the number of bottlenecks to be stacked\n",
    "        for i in range(self.n):\n",
    "            # apply the bottleneck to the first half channels and store them in bn_x\n",
    "            bn = self.bottleneck(bn)\n",
    "            bn_x.append(bn)\n",
    "               \n",
    "        # concatenate the first half channels with the second half channels\n",
    "        bn_x.append(split_x[1])\n",
    "        \n",
    "        # concatenate the bottlenecks\n",
    "        x = torch.cat(bn_x, dim=1)\n",
    "        \n",
    "        x = self.conv2(x)                \n",
    "        return x\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, reg_max, c_in):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(k=3, s=1, p=1, c_in=c_in, c_out=c_in)\n",
    "        self.conv2d_bbox = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=4*reg_max)\n",
    "        self.conv2d_cls = nn.Conv2d(kernel_size=1, stride=1, padding=0, in_channels=c_in, out_channels=num_classes)\n",
    "        self.linear = nn.Linear(in_features=int(c_in*c_in), out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv(x)\n",
    "        x_cls = self.conv2d_cls(x)\n",
    "        x_bbox = self.conv2d_bbox(x)\n",
    "\n",
    "        # flatten feature maps\n",
    "        x_cls = x_cls.reshape(x_cls.size(0), x_cls.size(1), -1)\n",
    "        x_bbox = x_bbox.reshape(x_bbox.size(0), x_bbox.size(1), -1)      \n",
    "        # apply sigmoid to the class scores\n",
    "        x_cls = torch.sigmoid(x_cls)\n",
    "        \n",
    "        # apply softmax to the class scores\n",
    "        print(x_cls.shape)\n",
    "        x_cls = F.softmax(x_cls, dim=1)\n",
    "        \n",
    "        # apply linear layer to the bbox coordinates\n",
    "        print(x_bbox.shape)\n",
    "        print(x_bbox.size(0), x_bbox.size(1), x_bbox.size(2))\n",
    "        val = self.linear(x_bbox)\n",
    "        print(val.shape)\n",
    "        print(\"flag\")\n",
    "        \n",
    "        return x_cls, x_bbox\n",
    "\n",
    "    def calculate_bbox(self, x):\n",
    "        t_x = x[:, 0, :, :]\n",
    "        t_y = x[:, 1, :, :]\n",
    "        t_w = x[:, 2, :, :]\n",
    "        t_h = x[:, 3, :, :]\n",
    "\n",
    "        pred_bbox = torch.cat((t_x, t_y, t_w, t_h), dim=0)\n",
    "\n",
    "        return pred_bbox\n",
    "\n",
    "\n",
    "# Main Network architecture\n",
    "class BBoxNet(nn.Module):\n",
    "    def __init__(self, w, r, d):\n",
    "        super(BBoxNet, self).__init__()\n",
    "    # backbone network modules\n",
    "        self.conv_0_p1 = Conv(k=3, s=2, p=1, c_in=3, c_out=int(64*w))\n",
    "        self.conv_1_p2 = Conv(k=3, s=2, p=1, c_in=int(64*w), c_out=int(128*w))\n",
    "        self.c2f_2 = C2f(shortcut=True, h=160, w=160, n=int(3*d), c_in=int(128*w), c_out=int(128*w))\n",
    "        self.conv_3_p3 = Conv(k=3, s=2, p=1, c_in=int(128*w), c_out=int(256*w))\n",
    "        self.c2f_4 = C2f(shortcut=True, h=80, w=80, n=int(6*d), c_in=int(256*w), c_out=int(256*w))\n",
    "        self.conv_5_p4 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(512*w))\n",
    "        self.c2f_6 = C2f(shortcut=True, h=40, w=40, n=int(6*d), c_in=int(512*w), c_out=int(512*w))\n",
    "        self.conv_7_p5 = Conv(k=3, s=2, p=1, c_in=int(512*w), c_out=int(512*w*r))\n",
    "        self.c2f_8 = C2f(shortcut=True, h=20, w=20, n=int(3*d), c_in=int(512*w*r), c_out=int(512*r*w))\n",
    "        self.sppf_9 = SPPF(c_in=int(512*w*r))\n",
    "\n",
    "    # head network modules\n",
    "        self.upsample_10 = nn.Upsample(size=(40,40), mode='bilinear', align_corners=False)\n",
    "        self.concat_11 = torch.cat\n",
    "        self.c2f_12 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.upsample_resolution_13a = nn.Upsample(size=(80,80), mode='bilinear', align_corners=False)\n",
    "        self.upsample_channels_13b = nn.Conv2d(in_channels=int(512*w), out_channels=int(256*w), kernel_size=1)\n",
    "        self.concat_14 = torch.cat\n",
    "        self.c2f_15 = C2f(shortcut=False, c_in=int(512*w), c_out=int(256*w), h=80, w=80, n=int(3*d))\n",
    "        self.conv_16_p3 = Conv(k=3, s=2, p=1, c_in=int(256*w), c_out=int(256*w))\n",
    "        self.concat_17 = torch.cat\n",
    "        # ISSUE HERE, THE ARCHITECTURE OUTPUT CHANNEL SIZE IS PROBABLY WRONG, AS THE CONCATENATION DOES NOT INCREASE THE CHANNEL SIZE\n",
    "        #self.c2f_18 = C2f(shortcut=False, c_in=int(512*w), c_out=int(512*w), h=40, w=40, n=int(3*d))\n",
    "        self.c2f_18 = C2f(shortcut=False, c_in=192, c_out=192, h=40, w=40, n=int(3*d))\n",
    "        self.conv_19 = Conv(k=3, s=2, p=1, c_in=192, c_out=192)\n",
    "        self.concat_20 = torch.cat\n",
    "        #self.c2f_21 = C2f(shortcut=False, c_in=int(512*w*(1+r)), c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "        self.c2f_21 = C2f(shortcut=False, c_in=448, c_out=int(512*w), h=20, w=20, n=int(3*d))\n",
    "    \n",
    "    # output layers\n",
    "        self.detect1 = Detect(num_classes=6, reg_max=1, c_in=int(256*w))\n",
    "        self.detect2 = Detect(num_classes=6, reg_max=1, c_in=192)\n",
    "        self.detect3 = Detect(num_classes=6, reg_max=1, c_in=int(512*w))\n",
    "\n",
    "    def forward(self,x):\n",
    "    # backbone pass\n",
    "        x = self.conv_0_p1(x)\n",
    "        x = self.conv_1_p2(x)\n",
    "        x = self.c2f_2(x)\n",
    "        x = self.conv_3_p3(x)\n",
    "        x = self.c2f_4(x)\n",
    "        # save for concat later\n",
    "        x_4 = x\n",
    "        \n",
    "        x = self.conv_5_p4(x)\n",
    "        x = self.c2f_6(x)\n",
    "        \n",
    "        # save for concat later\n",
    "        x_6 = x\n",
    "        x = self.conv_7_p5(x)\n",
    "        x = self.c2f_8(x)\n",
    "        x = self.sppf_9(x)\n",
    "        x_9 = x\n",
    "\n",
    "    # head pass\n",
    "        # first brancH\n",
    "        x = self.upsample_10(x)\n",
    "        x = self.concat_11((x, x_6), dim=1)\n",
    "        x = self.c2f_12(x)  \n",
    "        x_12 = x\n",
    "        x = self.upsample_resolution_13a(x)\n",
    "        x = self.upsample_channels_13b(x)\n",
    "        x = self.concat_14((x, x_4), dim=1)\n",
    "        x = self.c2f_15(x) \n",
    "        x_detect1 = x\n",
    "        \n",
    "    # second branch\n",
    "        x = self.conv_16_p3(x)\n",
    "        # CHECK CHANNEL ISSUE HEREISSUE HERE\n",
    "        x = self.concat_17((x_12, x), dim=1)\n",
    "        x = self.c2f_18(x)\n",
    "        x_detect2 = x\n",
    "        x = self.conv_19(x)\n",
    "        # ISSUE PROPAGATES HERE ALSO\n",
    "        x = self.concat_20((x, x_9), dim=1)\n",
    "        x = self.c2f_21(x)\n",
    "        x_detect3 = x\n",
    "    \n",
    "    # output layers\n",
    "        x_cls1, x_bbox1 = self.detect1(x_detect1)\n",
    "        #x_cls2, x_bbox2 = self.detect2(x_detect2)\n",
    "        #x_cls3, x_bbox3 = self.detect3(x_detect3)        \n",
    "\n",
    "        print(x_cls1.shape, x_bbox1.shape)\n",
    "        \n",
    "        pass\n",
    "        #return x_cls, x_bbox\n",
    "    \n",
    "def loss_bbox(output_bbox, target_bbox):\n",
    "    # output_bbox is a tensor of shape (batch_size, 12, 80, 80)\n",
    "    # we want to calculate the loss for each bounding box\n",
    "    print(\"test\")\n",
    "    p_x = output_bbox[:,0,:,:]\n",
    "    p_y = output_bbox[:,1,:,:]\n",
    "    p_w = output_bbox[:,2,:,:]\n",
    "    p_h = output_bbox[:,3,:,:]\n",
    "    \n",
    "    print(p_x.shape)\n",
    "    pass\n",
    "\n",
    "\n",
    "# define the training function\n",
    "def train_net(net, train_loader, n_epochs, optimizer, scheduler, criterion):\n",
    "    # loop over the number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # set the model to training mode\n",
    "        net.train()\n",
    "        for i_batch, data in enumerate(dataloader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data = data.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # show the image\n",
    "            #plt.imshow(data['image'][0])\n",
    "            #plt.show()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = net(data['image'].permute(0,3,1,2).float())\n",
    "            \n",
    "            # calculate the batch loss\n",
    "            print(output[1].shape)\n",
    "            break\n",
    "            # trasnpose the output to (batch_size, 80, 80, 12)\n",
    "            #output = (output[1].permute(0,2,3,1), output[1].permute(0,2,3,1))\n",
    "            #plt.imshow(output[1][0,:,:,0].detach().cpu().numpy())\n",
    "            #plt.show()\n",
    "\n",
    "#            loss = criterion(output[1], data['bbox'])\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "#            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # print training statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "\n",
    "        # step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "# initialize the network\n",
    "net = BBoxNet(w=0.25, r=2, d=0.34)\n",
    "# move the network to the GPU\n",
    "net = net.to(device)\n",
    "# define the loss function\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = loss_bbox\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "# define the scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# define the number of epochs\n",
    "n_epochs = 10\n",
    "# train the network\n",
    "train_net(net, dataloader, n_epochs, optimizer, scheduler, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

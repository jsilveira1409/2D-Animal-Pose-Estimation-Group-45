{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsilveira1409/CIVIL-459-Animal-Pose-Estimation/blob/sda-dev/openpifpaf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C6Q3KObSkLC",
        "outputId": "6af3f008-b7e3-4d86-cbcf-0bae0274aeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpifpaf in /usr/local/lib/python3.9/dist-packages (0.13.11)\n",
            "Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (8.4.0)\n",
            "Requirement already satisfied: pysparkling in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (0.6.2)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata!=3.8.0 in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (6.4.1)\n",
            "Requirement already satisfied: torchvision==0.14.1 in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (0.14.1)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.9/dist-packages (from openpifpaf) (2.0.7)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->openpifpaf) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->openpifpaf) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->openpifpaf) (8.5.0.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->openpifpaf) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->openpifpaf) (11.7.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1->openpifpaf) (2.27.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->openpifpaf) (67.7.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->openpifpaf) (0.40.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata!=3.8.0->openpifpaf) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2019.3 in /usr/local/lib/python3.9/dist-packages (from pysparkling->openpifpaf) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from pysparkling->openpifpaf) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.0->pysparkling->openpifpaf) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->openpifpaf) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->openpifpaf) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->openpifpaf) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->openpifpaf) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "!pip3 install openpifpaf\n",
        "\n",
        "import openpifpaf\n",
        "import IPython\n",
        "from openpifpaf.transforms import Crop\n",
        "from PIL import Image\n",
        "import gdown\n",
        "import subprocess\n",
        "openpifpaf.show.Canvas.show = True\n",
        "\n",
        "from openpifpaf_animalplugin import AnimalPoseEstimation\n",
        "#from openpifpaf_sdaplugin import SDA\n",
        "import coco_adapt as coco_adapt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNQyd4GCPGqb"
      },
      "source": [
        "# Dataset download and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y2-BLnTS5nl",
        "outputId": "2509a2c6-81d1-4df2-9ac4-813f916f8b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'plugins'...\n",
            "remote: Enumerating objects: 1858, done.\u001b[K\n",
            "remote: Counting objects: 100% (1713/1713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1490/1490), done.\u001b[K\n",
            "remote: Total 1858 (delta 182), reused 1694 (delta 166), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (1858/1858), 70.19 MiB | 24.76 MiB/s, done.\n",
            "Resolving deltas: 100% (227/227), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf plugins\n",
        "!rm -rf openpifpaf_animalplugin\n",
        "!git clone --branch sda-dev https://github.com/jsilveira1409/CIVIL-459-Animal-Pose-Estimation.git plugins\n",
        "!mv plugins/openpifpaf_animalplugin/ openpifpaf_animalplugin\n",
        "#!rm -rf data-animalpose/\n",
        "#!mkdir data-animalpose\n",
        "#!gdown \"https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x\" -O data-animalpose --folder \n",
        "#!unzip data-animalpose/images.zip -d data-animalpose/\n",
        "#!rm data-animalpose/images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcdkqDzFSkLF",
        "outputId": "83bc2893-9b80-440f-cdd7-5be31fc59878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'animal': openpifpaf.plugins.animalpose.animal_kp.AnimalKp,\n",
              " 'apollo': openpifpaf.plugins.apollocar3d.apollo_kp.ApolloKp,\n",
              " 'cifar10': openpifpaf.plugins.cifar10.datamodule.Cifar10,\n",
              " 'cocodet': openpifpaf.plugins.coco.cocodet.CocoDet,\n",
              " 'cocokp': openpifpaf.plugins.coco.cocokp.CocoKp,\n",
              " 'crowdpose': openpifpaf.plugins.crowdpose.module.CrowdPose,\n",
              " 'nuscenes': openpifpaf.plugins.nuscenes.nuscenes.NuScenes,\n",
              " 'posetrack2018': openpifpaf.plugins.posetrack.posetrack2018.Posetrack2018,\n",
              " 'posetrack2017': openpifpaf.plugins.posetrack.posetrack2017.Posetrack2017,\n",
              " 'cocokpst': openpifpaf.plugins.posetrack.cocokpst.CocoKpSt,\n",
              " 'wholebody': openpifpaf.plugins.wholebody.wholebody.Wholebody,\n",
              " 'custom_animal': openpifpaf_animalplugin.dataset.AnimalPoseEstimation}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "coco_adapt.adapt_to_coco()\n",
        "config = openpifpaf.plugin.register()\n",
        "openpifpaf.DATAMODULES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyGh5iBePGqc"
      },
      "source": [
        "# Animal Pose Estimation Dataset and Plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JRM-bJlYPGqd"
      },
      "outputs": [],
      "source": [
        "dataset = AnimalPoseEstimation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1LgcVosPGqd"
      },
      "source": [
        "# Semantic Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp-lTfEKPGqd",
        "outputId": "95764cf9-1402-4cf7-ae66-aaf223563867"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'AnimalPoseEstimation' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\openpifpaf.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/openpifpaf.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sda \u001b[39m=\u001b[39m SDA(output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata-animalpose/sda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/openpifpaf.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sda(dataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m], dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
            "\u001b[1;31mTypeError\u001b[0m: 'AnimalPoseEstimation' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "sda = SDA(output_dir='data-animalpose/sda')\n",
        "\n",
        "sda(dataset[0][0], dataset[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfieShnUPGqd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW6CLTFhSkLF",
        "outputId": "b3254aed-86cd-4413-cb38-25485d01e3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:neural network device: cpu (CUDA available: False, count: 0)\n",
            "INFO:openpifpaf.network.basenetworks:shufflenetv2k30: stride = 16, output features = 2048\n",
            "INFO:openpifpaf.network.losses.multi_head:multihead loss: ['custom-animal.cif.c', 'custom-animal.cif.vec', 'custom-animal.cif.scales', 'custom-animal.caf.c', 'custom-animal.caf.vec', 'custom-animal.caf.scales'], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "INFO:openpifpaf.logger:{'type': 'process', 'argv': ['/usr/local/lib/python3.9/dist-packages/openpifpaf/train.py', '--dataset', 'custom_animal', '--basenet=shufflenetv2k30', '--lr=0.00002', '--momentum=0.95', '--epochs=50', '--lr-decay', '160', '260', '--lr-decay-epochs=10', '--weight-decay=1e-5', '--weight-decay=1e-5', '--val-interval', '10', '--loader-workers', '2', '--batch-size', '11'], 'args': {'output': 'outputs/shufflenetv2k30-230425-073221-custom_animal.pkl', 'disable_cuda': False, 'ddp': False, 'local_rank': None, 'sync_batchnorm': True, 'quiet': False, 'debug': False, 'log_stats': False, 'mobilenetv3_pretrained': True, 'shufflenetv2_pretrained': True, 'shufflenetv2k_input_conv2_stride': 0, 'shufflenetv2k_input_conv2_outchannels': None, 'shufflenetv2k_stage4_dilation': 1, 'shufflenetv2k_kernel': 5, 'shufflenetv2k_conv5_as_stage': False, 'shufflenetv2k_instance_norm': False, 'shufflenetv2k_group_norm': False, 'shufflenetv2k_leaky_relu': False, 'mobilenetv2_pretrained': True, 'resnet_pretrained': True, 'resnet_pool0_stride': 0, 'resnet_input_conv_stride': 2, 'resnet_input_conv2_stride': 0, 'resnet_block5_dilation': 1, 'resnet_remove_last_block': False, 'cf4_dropout': 0.0, 'cf4_inplace_ops': True, 'checkpoint': None, 'basenet': 'shufflenetv2k30', 'cross_talk': 0.0, 'download_progress': True, 'head_consolidation': 'filter_and_extend', 'lambdas': None, 'component_lambdas': None, 'auto_tune_mtl': False, 'auto_tune_mtl_variance': False, 'task_sparsity_weight': 0.0, 'b_scale': 1.0, 'scale_log': False, 'scale_soft_clamp': 5.0, 'focal_alpha': 0.5, 'focal_gamma': 1.0, 'bce_soft_clamp': 5.0, 'bce_background_clamp': -15.0, 'regression_soft_clamp': 5.0, 'epochs': 50, 'train_batches': None, 'val_batches': None, 'clip_grad_norm': 0.0, 'clip_grad_value': 0.0, 'log_interval': 11, 'val_interval': 10, 'stride_apply': 1, 'fix_batch_norm': False, 'ema': 0.01, 'profile': None, 'cif_side_length': 4, 'caf_min_size': 3, 'caf_fixed_size': False, 'caf_aspect_ratio': 0.0, 'encoder_suppress_selfhidden': True, 'encoder_suppress_invisible': False, 'encoder_suppress_collision': False, 'momentum': 0.95, 'beta2': 0.999, 'adam_eps': 1e-06, 'nesterov': True, 'weight_decay': 1e-05, 'adam': False, 'amsgrad': False, 'lr': 2e-05, 'lr_decay': [160.0, 260.0], 'lr_decay_factor': 0.1, 'lr_decay_epochs': 10.0, 'lr_warm_up_start_epoch': 0, 'lr_warm_up_epochs': 1, 'lr_warm_up_factor': 0.001, 'lr_warm_restarts': [], 'lr_warm_restart_duration': 0.5, 'dataset': 'custom_animal', 'loader_workers': 2, 'batch_size': 11, 'dataset_weights': None, 'animal_train_annotations': 'data-animalpose/annotations/animal_keypoints_20_train.json', 'animal_val_annotations': 'data-animalpose/annotations/animal_keypoints_20_val.json', 'animal_train_image_dir': 'data-animalpose/images/train/', 'animal_val_image_dir': 'data-animalpose/images/val/', 'animal_square_edge': 513, 'animal_extended_scale': False, 'animal_orientation_invariant': 0.0, 'animal_blur': 0.0, 'animal_augmentation': True, 'animal_rescale_images': 1.0, 'animal_upsample': 1, 'animal_min_kp_anns': 1, 'animal_bmin': 1, 'animal_eval_test2017': False, 'animal_eval_testdev2017': False, 'animal_eval_annotation_filter': True, 'animal_eval_long_edge': 0, 'animal_eval_extended_scale': False, 'animal_eval_orientation_invariant': 0.0, 'apollo_train_annotations': 'data-apollocar3d/annotations/apollo_keypoints_66_train.json', 'apollo_val_annotations': 'data-apollocar3d/annotations/apollo_keypoints_66_val.json', 'apollo_train_image_dir': 'data-apollocar3d/images/train/', 'apollo_val_image_dir': 'data-apollocar3d/images/val/', 'apollo_square_edge': 513, 'apollo_extended_scale': False, 'apollo_orientation_invariant': 0.0, 'apollo_blur': 0.0, 'apollo_augmentation': True, 'apollo_rescale_images': 1.0, 'apollo_upsample': 1, 'apollo_min_kp_anns': 1, 'apollo_bmin': 1, 'apollo_apply_local_centrality': False, 'apollo_eval_annotation_filter': True, 'apollo_eval_long_edge': 0, 'apollo_eval_extended_scale': False, 'apollo_eval_orientation_invariant': 0.0, 'apollo_use_24_kps': False, 'cifar10_root_dir': 'data-cifar10/', 'cifar10_download': False, 'cocodet_train_annotations': 'data-mscoco/annotations/instances_train2017.json', 'cocodet_val_annotations': 'data-mscoco/annotations/instances_val2017.json', 'cocodet_train_image_dir': 'data-mscoco/images/train2017/', 'cocodet_val_image_dir': 'data-mscoco/images/val2017/', 'cocodet_square_edge': 513, 'cocodet_extended_scale': False, 'cocodet_orientation_invariant': 0.0, 'cocodet_blur': 0.0, 'cocodet_augmentation': True, 'cocodet_rescale_images': 1.0, 'cocodet_upsample': 1, 'cocokp_train_annotations': 'data-mscoco/annotations/person_keypoints_train2017.json', 'cocokp_val_annotations': 'data-mscoco/annotations/person_keypoints_val2017.json', 'cocokp_train_image_dir': 'data-mscoco/images/train2017/', 'cocokp_val_image_dir': 'data-mscoco/images/val2017/', 'cocokp_square_edge': 385, 'cocokp_with_dense': False, 'cocokp_extended_scale': False, 'cocokp_orientation_invariant': 0.0, 'cocokp_blur': 0.0, 'cocokp_augmentation': True, 'cocokp_rescale_images': 1.0, 'cocokp_upsample': 1, 'cocokp_min_kp_anns': 1, 'cocokp_bmin': 0.1, 'cocokp_eval_test2017': False, 'cocokp_eval_testdev2017': False, 'coco_eval_annotation_filter': True, 'coco_eval_long_edge': 641, 'coco_eval_extended_scale': False, 'coco_eval_orientation_invariant': 0.0, 'crowdpose_train_annotations': 'data-crowdpose/json/crowdpose_train.json', 'crowdpose_val_annotations': 'data-crowdpose/json/crowdpose_val.json', 'crowdpose_image_dir': 'data-crowdpose/images/', 'crowdpose_square_edge': 385, 'crowdpose_extended_scale': False, 'crowdpose_orientation_invariant': 0.0, 'crowdpose_augmentation': True, 'crowdpose_rescale_images': 1.0, 'crowdpose_upsample': 1, 'crowdpose_min_kp_anns': 1, 'crowdpose_eval_test': False, 'crowdpose_eval_long_edge': 641, 'crowdpose_eval_extended_scale': False, 'crowdpose_eval_orientation_invariant': 0.0, 'crowdpose_index': None, 'nuscenes_train_annotations': '../../../NuScenes/mscoco_style_annotations/nuimages_v1.0-train.json', 'nuscenes_val_annotations': '../../../NuScenes/mscoco_style_annotations/nuimages_v1.0-val.json', 'nuscenes_train_image_dir': '../../../NuScenes/nuimages-v1.0-all-samples', 'nuscenes_val_image_dir': '../../../NuScenes/nuimages-v1.0-all-samples', 'nuscenes_square_edge': 513, 'nuscenes_extended_scale': False, 'nuscenes_orientation_invariant': 0.0, 'nuscenes_blur': 0.0, 'nuscenes_augmentation': True, 'nuscenes_rescale_images': 1.0, 'nuscenes_upsample': 1, 'posetrack2018_train_annotations': 'data-posetrack2018/annotations/train/*.json', 'posetrack2018_val_annotations': 'data-posetrack2018/annotations/val/*.json', 'posetrack2018_eval_annotations': 'data-posetrack2018/annotations/val/*.json', 'posetrack2018_data_root': 'data-posetrack2018', 'posetrack_square_edge': 385, 'posetrack_with_dense': False, 'posetrack_augmentation': True, 'posetrack_rescale_images': 1.0, 'posetrack_upsample': 1, 'posetrack_min_kp_anns': 1, 'posetrack_bmin': 0.1, 'posetrack_sample_pairing': 0.0, 'posetrack_image_augmentations': 0.0, 'posetrack_max_shift': 30.0, 'posetrack_eval_long_edge': 801, 'posetrack_eval_extended_scale': False, 'posetrack_eval_orientation_invariant': 0.0, 'posetrack_ablation_without_tcaf': False, 'posetrack2017_eval_annotations': 'data-posetrack2017/annotations/val/*.json', 'posetrack2017_data_root': 'data-posetrack2017', 'cocokpst_max_shift': 30.0, 'wholebody_train_annotations': 'data-mscoco/annotations/person_keypoints_train2017_wholebody_pifpaf_style.json', 'wholebody_val_annotations': 'data-mscoco/annotations/coco_wholebody_val_v1.0.json', 'wholebody_train_image_dir': 'data-mscoco/images/train2017/', 'wholebody_val_image_dir': 'data-mscoco/images/val2017', 'wholebody_square_edge': 385, 'wholebody_extended_scale': False, 'wholebody_orientation_invariant': 0.0, 'wholebody_blur': 0.0, 'wholebody_augmentation': True, 'wholebody_rescale_images': 1.0, 'wholebody_upsample': 1, 'wholebody_min_kp_anns': 1, 'wholebody_bmin': 1.0, 'wholebody_apply_local_centrality': False, 'wholebody_eval_test2017': False, 'wholebody_eval_testdev2017': False, 'wholebody_eval_annotation_filter': True, 'wholebody_eval_long_edge': 641, 'wholebody_eval_extended_scale': False, 'wholebody_eval_orientation_invariant': 0.0, 'custom_animal_image_dir': 'data-animalpose/images', 'custom_animal_annotation_file': 'data-animalpose/keypoints.json', 'custom_animal_extended_scale': False, 'custom_animal_orientation_invariant': 0.0, 'custom_animal_blur': 0.0, 'custom_animal_rescale_images': 1.0, 'custom_animal_upsample': 1, 'custom_animal_min_kp_anns': 1, 'custom_animal_bmin': 1, 'save_all': None, 'show': False, 'image_width': None, 'image_height': None, 'image_dpi_factor': 2.0, 'image_min_dpi': 50.0, 'show_file_extension': 'jpeg', 'textbox_alpha': 0.5, 'text_color': 'white', 'font_size': 8, 'monocolor_connections': False, 'line_width': None, 'skeleton_solid_threshold': 0.5, 'show_box': False, 'white_overlay': False, 'show_joint_scales': False, 'show_joint_confidences': False, 'show_decoding_order': False, 'show_frontier_order': False, 'show_only_decoded_connections': False, 'video_fps': 10, 'video_dpi': 100, 'debug_indices': [], 'device': device(type='cpu'), 'pin_memory': False}, 'version': '0.13.11', 'plugin_versions': {'openpifpaf_animalplugin': 'unknown'}, 'hostname': 'cac20186c6cc'}\n",
            "loading annotations into memory...\n",
            "Done (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "INFO:openpifpaf.plugins.coco.dataset:Images: 4608\n",
            "loading annotations into memory...\n",
            "Done (t=0.18s)\n",
            "creating index...\n",
            "index created!\n",
            "INFO:openpifpaf.plugins.coco.dataset:Images: 4608\n",
            "INFO:openpifpaf.optimize:SGD optimizer\n",
            "INFO:openpifpaf.optimize:training batches per epoch = 418\n",
            "INFO:openpifpaf.network.trainer:{'type': 'config', 'field_names': ['custom-animal.cif.c', 'custom-animal.cif.vec', 'custom-animal.cif.scales', 'custom-animal.caf.c', 'custom-animal.caf.vec', 'custom-animal.caf.scales']}\n",
            "INFO:openpifpaf.network.trainer:model written: outputs/shufflenetv2k30-230425-073221-custom_animal.pkl.epoch000\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 -m openpifpaf.train \\\n",
        "    --dataset custom_animal \\\n",
        "    --basenet=shufflenetv2k30 \\\n",
        "    --lr=0.00002 \\\n",
        "    --momentum=0.95 \\\n",
        "    --epochs=50 \\\n",
        "    --lr-decay 160 260 \\\n",
        "    --lr-decay-epochs=10  \\\n",
        "    --weight-decay=1e-5 \\\n",
        "    --weight-decay=1e-5 \\\n",
        "    --val-interval 10 \\\n",
        "    --loader-workers 2 \\\n",
        "    --batch-size 11"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "\n",
        "# create a zip file of the folder you want to save\n",
        "folder_path = '/outputs'\n",
        "zip_path = '/content/myfolder.zip'\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file))\n",
        "\n",
        "# save the zip file to your Google Drive\n",
        "drive_path = '/content/drive/My Drive/myfolder.zip'\n",
        "!cp '{zip_path}' '{drive_path}'"
      ],
      "metadata": {
        "id": "dPqW8AXHQvkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t  dev\tlib    libx32  notebooks  root\tsrv\t tmp\n",
      "boot\t  etc\tlib32  media   opt\t  run\tstorage  usr\n",
      "datasets  home\tlib64  mnt     proc\t  sbin\tsys\t var\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip3 install openpifpaf\n",
    "    !rm -rf plugins\n",
    "    !rm -rf openpifpaf_animalplugin\n",
    "    !rm -rf openpifpaf_sdaplugin\n",
    "    !git clone --branch main https://github.com/jsilveira1409/CIVIL-459-Animal-Pose-Estimation.git plugins\n",
    "    !mv plugins/openpifpaf_animalpose2/ openpifpaf_animalpose\n",
    "    !mv plugins/openpifpaf_sdaplugin/ openpifpaf_sdaplugin\n",
    "    !rm -rf plugins\n",
    "\n",
    "import openpifpaf\n",
    "from openpifpaf_sdaplugin import SDA\n",
    "import numpy as np\n",
    "\n",
    "#from openpifpaf.plugins.animalpose import AnimalKp\n",
    "from openpifpaf_animalpose2.animal_kp_custom import AnimalKpCustom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import gdown\n",
    "import subprocess\n",
    "import argparse\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "openpifpaf.show.Canvas.show = True\n",
    "\n",
    "train_annotations = 'data-animalpose/annotations/animal_keypoints_20_train.json'\n",
    "val_annotations = 'data-animalpose/annotations/animal_keypoints_20_val.json'\n",
    "eval_annotations = val_annotations\n",
    "train_image_dir = 'data-animalpose/images/train/'\n",
    "val_image_dir = 'data-animalpose/images/val/'\n",
    "eval_image_dir = val_image_dir\n",
    "\n",
    "keypoint_file = 'data-animalpose/keypoints.json'\n",
    "images_folder = 'data-animalpose/images/'\n",
    "\n",
    "def download_dataset():\n",
    "    cmd = 'rm -rf data-animalpose'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'mkdir data-animalpose'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'gdown \"https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x\" -O data-animalpose --folder'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'mkdir data-animalpose/output'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'unzip data-animalpose/images.zip -d data-animalpose/'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'rm data-animalpose/images.zip'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    print(\"Downloaded dataset\")\n",
    "\n",
    "def convert_keypoints_format(keypoints_list):\n",
    "    keypoints_flat = []\n",
    "    for keypoint in keypoints_list:\n",
    "        keypoints_flat.extend([k for k in keypoint])\n",
    "    return keypoints_flat\n",
    "\n",
    "def adapt_to_coco():\n",
    "    # Load the input JSON file\n",
    "    with open(keypoint_file, 'r') as f:\n",
    "        input_dict = json.load(f)\n",
    "\n",
    "    # Create a copy of the input dictionary\n",
    "    output_dict = copy.deepcopy(input_dict)\n",
    "\n",
    "    # Update images list format\n",
    "    images_list = []\n",
    "    for image_id, image_filename in input_dict['images'].items():\n",
    "        images_list.append({'id': int(image_id), 'file_name': image_filename})\n",
    "        #images_list.append({'id': image_id, image_id: image_filename})\n",
    "    output_dict['images'] = images_list\n",
    "\n",
    "    # Update annotations keypoints format and add missing fields\n",
    "    annotations_list = []\n",
    "    for i, annotation in enumerate(input_dict['annotations']):\n",
    "        new_annotation = {\n",
    "            'id': i + 1,\n",
    "            'image_id': annotation['image_id'],\n",
    "            'category_id': annotation['category_id'],\n",
    "            'bbox': annotation['bbox'],\n",
    "            'keypoints': convert_keypoints_format(annotation['keypoints']),\n",
    "            'num_keypoints': annotation['num_keypoints'],\n",
    "            'iscrowd': 0,\n",
    "        }\n",
    "        annotations_list.append(new_annotation)\n",
    "    output_dict['annotations'] = annotations_list\n",
    "\n",
    "    # Save the converted data to a JSON file\n",
    "    with open(keypoint_file, 'w') as f:\n",
    "        json.dump(output_dict, f, indent=4)\n",
    "    print(\"Converted to COCO format\")\n",
    "\n",
    "def split_data():\n",
    "    # Load the input JSON file\n",
    "    with open(keypoint_file, 'r') as f:\n",
    "        input_dict = json.load(f)\n",
    "\n",
    "    # Create a copy of the input dictionary\n",
    "    output_dict = copy.deepcopy(input_dict)\n",
    "\n",
    "    # Split the data into train and val\n",
    "    train_dict = copy.deepcopy(output_dict)\n",
    "    val_dict = copy.deepcopy(output_dict)\n",
    "\n",
    "    # Update images list format\n",
    "    train_images_list = []\n",
    "    val_images_list = []\n",
    "    for image in output_dict['images']:\n",
    "        if random.random() < 0.8:  # 80% probability for train, 20% for validation\n",
    "            train_images_list.append(image)\n",
    "        else:\n",
    "            val_images_list.append(image)\n",
    "    train_dict['images'] = train_images_list\n",
    "    val_dict['images'] = val_images_list\n",
    "\n",
    "    # Update annotations keypoints format and add missing fields\n",
    "    train_annotations_list = []\n",
    "    val_annotations_list = []\n",
    "    for annotation in output_dict['annotations']:\n",
    "        if annotation['image_id'] in [img['id'] for img in train_images_list]:\n",
    "            train_annotations_list.append(annotation)\n",
    "        elif annotation['image_id'] in [img['id'] for img in val_images_list]:\n",
    "            val_annotations_list.append(annotation)\n",
    "    train_dict['annotations'] = train_annotations_list\n",
    "    val_dict['annotations'] = val_annotations_list\n",
    "    \n",
    "    # create files if not already existant\n",
    "    if not os.path.exists(train_image_dir):\n",
    "        os.makedirs(train_image_dir)\n",
    "    if not os.path.exists(val_image_dir):\n",
    "        os.makedirs(val_image_dir)\n",
    "    if not os.path.exists(eval_image_dir):\n",
    "        os.makedirs(eval_image_dir)\n",
    "    if not os.path.exists('data-animalpose/annotations/'):\n",
    "        os.makedirs('data-animalpose/annotations/')\n",
    "\n",
    "    # create train_annotations file\n",
    "    if not os.path.exists(train_annotations):\n",
    "        open(train_annotations, 'w').close()\n",
    "    # create val_annotations file\n",
    "    if not os.path.exists(val_annotations):\n",
    "        open(val_annotations, 'w').close()\n",
    "    # create eval_annotations file\n",
    "    if not os.path.exists(eval_annotations):\n",
    "        open(eval_annotations, 'w').close()\n",
    "\n",
    "        \n",
    "    # Save the converted data to a JSON file\n",
    "    \n",
    "    with open(train_annotations, 'w') as f:\n",
    "        json.dump(train_dict, f, indent=4)\n",
    "    with open(eval_annotations, 'w') as f:\n",
    "        json.dump(val_dict, f, indent=4)\n",
    "\n",
    "\n",
    "    # move images to train and val folders\n",
    "    for image in train_images_list:\n",
    "        #file_name = image[str(image['id'])]\n",
    "        file_name = image['file_name']\n",
    "        os.rename(images_folder + file_name, train_image_dir + file_name)\n",
    "    for image in val_images_list:\n",
    "        #file_name = image[str(image['id'])]\n",
    "        file_name = image['file_name']\n",
    "        os.rename(images_folder + file_name, val_image_dir + file_name)\n",
    "    print(\"Split data into train and val\")\n",
    "\n",
    "def test_sda (sda):\n",
    "    img = Image.open('data-animalpose/images/train/2007_001397.jpg')\n",
    "    tensor_img = np.array(img)\n",
    "    img1 = sda.apply(tensor_img)\n",
    "    plt.imshow(tensor_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dataset\n",
      "Converted to COCO format\n",
      "Split data into train and val\n",
      "sdaplugin init\n",
      "Cropped dataset\n",
      "{'animal': <class 'openpifpaf.plugins.animalpose.animal_kp.AnimalKp'>, 'apollo': <class 'openpifpaf.plugins.apollocar3d.apollo_kp.ApolloKp'>, 'cifar10': <class 'openpifpaf.plugins.cifar10.datamodule.Cifar10'>, 'cocodet': <class 'openpifpaf.plugins.coco.cocodet.CocoDet'>, 'cocokp': <class 'openpifpaf.plugins.coco.cocokp.CocoKp'>, 'crowdpose': <class 'openpifpaf.plugins.crowdpose.module.CrowdPose'>, 'nuscenes': <class 'openpifpaf.plugins.nuscenes.nuscenes.NuScenes'>, 'posetrack2018': <class 'openpifpaf.plugins.posetrack.posetrack2018.Posetrack2018'>, 'posetrack2017': <class 'openpifpaf.plugins.posetrack.posetrack2017.Posetrack2017'>, 'cocokpst': <class 'openpifpaf.plugins.posetrack.cocokpst.CocoKpSt'>, 'wholebody': <class 'openpifpaf.plugins.wholebody.wholebody.Wholebody'>, 'custom_animal': <class 'openpifpaf_animalpose2.animal_kp_custom.AnimalKpCustom'>, 'sda': <class 'openpifpaf_sdaplugin.sda.SDA'>}\n"
     ]
    }
   ],
   "source": [
    " # 1. Download dataset\n",
    "download_dataset()\n",
    "# 2. Convert to COCO format \n",
    "adapt_to_coco()\n",
    "# 3. Split data into train and val\n",
    "split_data()\n",
    "\n",
    "# 4. Initialize SDA and crop the dataset, creating a body part pool\n",
    "sda = SDA()\n",
    "sda.crop_dataset()\n",
    "print(\"Cropped dataset\")\n",
    "\n",
    "# 5. Configure plugins\n",
    "config = openpifpaf.plugin.register()\n",
    "print(openpifpaf.DATAMODULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdaplugin init\n",
      "INFO:__main__:neural network device: cpu (CUDA available: False, count: 0)\n",
      "INFO:openpifpaf.network.factory:filtering for dataset heads and extending existing heads\n",
      "INFO:openpifpaf.network.losses.multi_head:multihead loss: ['animal.cif.c', 'animal.cif.vec', 'animal.cif.scales', 'animal.caf.c', 'animal.caf.vec', 'animal.caf.scales'], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "INFO:openpifpaf.logger:{'type': 'process', 'argv': ['C:\\\\Users\\\\osour\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\openpifpaf\\\\train.py', '--lr=0.0003', '--momentum=0.95', '--clip-grad-value=10.0', '--b-scale=10.0', '--batch-size=8', '--loader-workers=12', '--epochs=350', '--lr-decay', '310', '330', '--lr-decay-epochs=10', '--val-interval', '5', '--checkpoint=shufflenetv2k30', '--lr-warm-up-start-epoch=200', '--dataset=animal', '--animal-square-edge=385', '--animal-upsample=2', '--animal-bmin=2', '--animal-extended-scale', '--weight-decay=1e-5'], 'args': {'output': 'outputs/shufflenetv2k30-230513-152739-animal.pkl', 'disable_cuda': False, 'ddp': False, 'local_rank': None, 'sync_batchnorm': True, 'quiet': False, 'debug': False, 'log_stats': False, 'mobilenetv2_pretrained': True, 'shufflenetv2k_input_conv2_stride': 0, 'shufflenetv2k_input_conv2_outchannels': None, 'shufflenetv2k_stage4_dilation': 1, 'shufflenetv2k_kernel': 5, 'shufflenetv2k_conv5_as_stage': False, 'shufflenetv2k_instance_norm': False, 'shufflenetv2k_group_norm': False, 'shufflenetv2k_leaky_relu': False, 'shufflenetv2_pretrained': True, 'resnet_pretrained': True, 'resnet_pool0_stride': 0, 'resnet_input_conv_stride': 2, 'resnet_input_conv2_stride': 0, 'resnet_block5_dilation': 1, 'resnet_remove_last_block': False, 'mobilenetv3_pretrained': True, 'cf4_dropout': 0.0, 'cf4_inplace_ops': True, 'checkpoint': 'shufflenetv2k30', 'basenet': None, 'cross_talk': 0.0, 'download_progress': True, 'head_consolidation': 'filter_and_extend', 'lambdas': None, 'component_lambdas': None, 'auto_tune_mtl': False, 'auto_tune_mtl_variance': False, 'task_sparsity_weight': 0.0, 'focal_alpha': 0.5, 'focal_gamma': 1.0, 'bce_soft_clamp': 5.0, 'bce_background_clamp': -15.0, 'regression_soft_clamp': 5.0, 'b_scale': 10.0, 'scale_log': False, 'scale_soft_clamp': 5.0, 'epochs': 350, 'train_batches': None, 'val_batches': None, 'clip_grad_norm': 0.0, 'clip_grad_value': 10.0, 'log_interval': 11, 'val_interval': 5, 'stride_apply': 1, 'fix_batch_norm': False, 'ema': 0.01, 'profile': None, 'cif_side_length': 4, 'caf_min_size': 3, 'caf_fixed_size': False, 'caf_aspect_ratio': 0.0, 'encoder_suppress_selfhidden': True, 'encoder_suppress_invisible': False, 'encoder_suppress_collision': False, 'momentum': 0.95, 'beta2': 0.999, 'adam_eps': 1e-06, 'nesterov': True, 'weight_decay': 1e-05, 'adam': False, 'amsgrad': False, 'lr': 0.0003, 'lr_decay': [310.0, 330.0], 'lr_decay_factor': 0.1, 'lr_decay_epochs': 10.0, 'lr_warm_up_start_epoch': 200.0, 'lr_warm_up_epochs': 1, 'lr_warm_up_factor': 0.001, 'lr_warm_restarts': [], 'lr_warm_restart_duration': 0.5, 'dataset': 'animal', 'loader_workers': 12, 'batch_size': 8, 'dataset_weights': None, 'animal_train_annotations': 'data-animalpose/annotations/animal_keypoints_20_train.json', 'animal_val_annotations': 'data-animalpose/annotations/animal_keypoints_20_val.json', 'animal_train_image_dir': 'data-animalpose/images/train/', 'animal_val_image_dir': 'data-animalpose/images/val/', 'animal_square_edge': 385, 'animal_extended_scale': True, 'animal_orientation_invariant': 0.0, 'animal_blur': 0.0, 'animal_augmentation': True, 'animal_rescale_images': 1.0, 'animal_upsample': 2, 'animal_min_kp_anns': 1, 'animal_bmin': 2, 'animal_eval_test2017': False, 'animal_eval_testdev2017': False, 'animal_eval_annotation_filter': True, 'animal_eval_long_edge': 0, 'animal_eval_extended_scale': False, 'animal_eval_orientation_invariant': 0.0, 'apollo_train_annotations': 'data-apollocar3d/annotations/apollo_keypoints_66_train.json', 'apollo_val_annotations': 'data-apollocar3d/annotations/apollo_keypoints_66_val.json', 'apollo_train_image_dir': 'data-apollocar3d/images/train/', 'apollo_val_image_dir': 'data-apollocar3d/images/val/', 'apollo_square_edge': 513, 'apollo_extended_scale': False, 'apollo_orientation_invariant': 0.0, 'apollo_blur': 0.0, 'apollo_augmentation': True, 'apollo_rescale_images': 1.0, 'apollo_upsample': 1, 'apollo_min_kp_anns': 1, 'apollo_bmin': 1, 'apollo_apply_local_centrality': False, 'apollo_eval_annotation_filter': True, 'apollo_eval_long_edge': 0, 'apollo_eval_extended_scale': False, 'apollo_eval_orientation_invariant': 0.0, 'apollo_use_24_kps': False, 'cifar10_root_dir': 'data-cifar10/', 'cifar10_download': False, 'cocodet_train_annotations': 'data-mscoco/annotations/instances_train2017.json', 'cocodet_val_annotations': 'data-mscoco/annotations/instances_val2017.json', 'cocodet_train_image_dir': 'data-mscoco/images/train2017/', 'cocodet_val_image_dir': 'data-mscoco/images/val2017/', 'cocodet_square_edge': 513, 'cocodet_extended_scale': False, 'cocodet_orientation_invariant': 0.0, 'cocodet_blur': 0.0, 'cocodet_augmentation': True, 'cocodet_rescale_images': 1.0, 'cocodet_upsample': 1, 'cocokp_train_annotations': 'data-mscoco/annotations/person_keypoints_train2017.json', 'cocokp_val_annotations': 'data-mscoco/annotations/person_keypoints_val2017.json', 'cocokp_train_image_dir': 'data-mscoco/images/train2017/', 'cocokp_val_image_dir': 'data-mscoco/images/val2017/', 'cocokp_square_edge': 385, 'cocokp_with_dense': False, 'cocokp_extended_scale': False, 'cocokp_orientation_invariant': 0.0, 'cocokp_blur': 0.0, 'cocokp_augmentation': True, 'cocokp_rescale_images': 1.0, 'cocokp_upsample': 1, 'cocokp_min_kp_anns': 1, 'cocokp_bmin': 0.1, 'cocokp_eval_test2017': False, 'cocokp_eval_testdev2017': False, 'coco_eval_annotation_filter': True, 'coco_eval_long_edge': 641, 'coco_eval_extended_scale': False, 'coco_eval_orientation_invariant': 0.0, 'crowdpose_train_annotations': 'data-crowdpose/json/crowdpose_train.json', 'crowdpose_val_annotations': 'data-crowdpose/json/crowdpose_val.json', 'crowdpose_image_dir': 'data-crowdpose/images/', 'crowdpose_square_edge': 385, 'crowdpose_extended_scale': False, 'crowdpose_orientation_invariant': 0.0, 'crowdpose_augmentation': True, 'crowdpose_rescale_images': 1.0, 'crowdpose_upsample': 1, 'crowdpose_min_kp_anns': 1, 'crowdpose_eval_test': False, 'crowdpose_eval_long_edge': 641, 'crowdpose_eval_extended_scale': False, 'crowdpose_eval_orientation_invariant': 0.0, 'crowdpose_index': None, 'nuscenes_train_annotations': '../../../NuScenes/mscoco_style_annotations/nuimages_v1.0-train.json', 'nuscenes_val_annotations': '../../../NuScenes/mscoco_style_annotations/nuimages_v1.0-val.json', 'nuscenes_train_image_dir': '../../../NuScenes/nuimages-v1.0-all-samples', 'nuscenes_val_image_dir': '../../../NuScenes/nuimages-v1.0-all-samples', 'nuscenes_square_edge': 513, 'nuscenes_extended_scale': False, 'nuscenes_orientation_invariant': 0.0, 'nuscenes_blur': 0.0, 'nuscenes_augmentation': True, 'nuscenes_rescale_images': 1.0, 'nuscenes_upsample': 1, 'posetrack2018_train_annotations': 'data-posetrack2018/annotations/train/*.json', 'posetrack2018_val_annotations': 'data-posetrack2018/annotations/val/*.json', 'posetrack2018_eval_annotations': 'data-posetrack2018/annotations/val/*.json', 'posetrack2018_data_root': 'data-posetrack2018', 'posetrack_square_edge': 385, 'posetrack_with_dense': False, 'posetrack_augmentation': True, 'posetrack_rescale_images': 1.0, 'posetrack_upsample': 1, 'posetrack_min_kp_anns': 1, 'posetrack_bmin': 0.1, 'posetrack_sample_pairing': 0.0, 'posetrack_image_augmentations': 0.0, 'posetrack_max_shift': 30.0, 'posetrack_eval_long_edge': 801, 'posetrack_eval_extended_scale': False, 'posetrack_eval_orientation_invariant': 0.0, 'posetrack_ablation_without_tcaf': False, 'posetrack2017_eval_annotations': 'data-posetrack2017/annotations/val/*.json', 'posetrack2017_data_root': 'data-posetrack2017', 'cocokpst_max_shift': 30.0, 'wholebody_train_annotations': 'data-mscoco/annotations/person_keypoints_train2017_wholebody_pifpaf_style.json', 'wholebody_val_annotations': 'data-mscoco/annotations/coco_wholebody_val_v1.0.json', 'wholebody_train_image_dir': 'data-mscoco/images/train2017/', 'wholebody_val_image_dir': 'data-mscoco/images/val2017', 'wholebody_square_edge': 385, 'wholebody_extended_scale': False, 'wholebody_orientation_invariant': 0.0, 'wholebody_blur': 0.0, 'wholebody_augmentation': True, 'wholebody_rescale_images': 1.0, 'wholebody_upsample': 1, 'wholebody_min_kp_anns': 1, 'wholebody_bmin': 1.0, 'wholebody_apply_local_centrality': False, 'wholebody_eval_test2017': False, 'wholebody_eval_testdev2017': False, 'wholebody_eval_annotation_filter': True, 'wholebody_eval_long_edge': 641, 'wholebody_eval_extended_scale': False, 'wholebody_eval_orientation_invariant': 0.0, 'custom_animal_train_annotations': 'data-animalpose/annotations/animal_keypoints_20_train.json', 'custom_animal_val_annotations': 'data-animalpose/annotations/animal_keypoints_20_val.json', 'custom_animal_train_image_dir': 'data-animalpose/images/train/', 'custom_animal_val_image_dir': 'data-animalpose/images/val/', 'custom_animal_square_edge': 513, 'custom_animal_extended_scale': False, 'custom_animal_orientation_invariant': 0.0, 'custom_animal_blur': 0.0, 'custom_animal_rescale_images': 1.0, 'custom_animal_upsample': 1, 'custom_animal_min_kp_anns': 1, 'custom_animal_bmin': 1, 'custom_animal_eval_test2017': False, 'custom_animal_eval_testdev2017': False, 'save_all': None, 'show': False, 'image_width': None, 'image_height': None, 'image_dpi_factor': 2.0, 'image_min_dpi': 50.0, 'show_file_extension': 'jpeg', 'textbox_alpha': 0.5, 'text_color': 'white', 'font_size': 8, 'monocolor_connections': False, 'line_width': None, 'skeleton_solid_threshold': 0.5, 'show_box': False, 'white_overlay': False, 'show_joint_scales': False, 'show_joint_confidences': False, 'show_decoding_order': False, 'show_frontier_order': False, 'show_only_decoded_connections': False, 'video_fps': 10, 'video_dpi': 100, 'debug_indices': [], 'device': device(type='cpu'), 'pin_memory': False}, 'version': '0.13.11', 'plugin_versions': {'openpifpaf_animalpose': 'unknown', 'openpifpaf_animalpose2': 'unknown', 'openpifpaf_sdaplugin': 'unknown'}, 'hostname': 'w11'}\n",
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO:openpifpaf.plugins.coco.dataset:filter for annotations (min kp=1) ...\n",
      "INFO:openpifpaf.plugins.coco.dataset:... done.\n",
      "INFO:openpifpaf.plugins.coco.dataset:Images: 1197\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO:openpifpaf.plugins.coco.dataset:filter for annotations (min kp=1) ...\n",
      "INFO:openpifpaf.plugins.coco.dataset:... done.\n",
      "INFO:openpifpaf.plugins.coco.dataset:Images: 332\n",
      "INFO:openpifpaf.optimize:SGD optimizer\n",
      "INFO:openpifpaf.optimize:training batches per epoch = 149\n",
      "INFO:openpifpaf.network.trainer:{'type': 'config', 'field_names': ['animal.cif.c', 'animal.cif.vec', 'animal.cif.scales', 'animal.caf.c', 'animal.caf.vec', 'animal.caf.scales']}\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "sdaplugin init\n",
      "INFO:openpifpaf.network.trainer:{'type': 'train', 'epoch': 200, 'batch': 0, 'n_batches': 149, 'time': 76.205, 'data_time': 36.823, 'lr': 3e-07, 'loss': 4899.237, 'head_losses': [1579.576, 479.338, 0.42, 1451.658, 1387.502, 0.743]}\n",
      "INFO:openpifpaf.network.trainer:{'type': 'train', 'epoch': 200, 'batch': 11, 'n_batches': 149, 'time': 32.429, 'data_time': 0.01, 'lr': 5e-07, 'loss': 3588.255, 'head_losses': [1079.836, 303.362, 0.621, 925.0, 1278.344, 1.093]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\train.py\", line 196, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\train.py\", line 192, in main\n",
      "    trainer.loop(train_loader, val_loader, start_epoch=start_epoch)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\network\\trainer.py\", line 158, in loop\n",
      "    self.train(train_scenes, epoch)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\network\\trainer.py\", line 289, in train\n",
      "    for batch_idx, (data, target, _) in enumerate(scenes):\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1313, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1359, in _process_data\n",
      "    data.reraise()\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_utils.py\", line 543, in reraise\n",
      "    raise exception\n",
      "ValueError: Caught ValueError in DataLoader worker process 5.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\plugins\\coco\\dataset.py\", line 129, in __getitem__\n",
      "    image, anns, meta = self.preprocess(image, anns, meta)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\transforms\\compose.py\", line 16, in __call__\n",
      "    args = p(*args)\n",
      "  File \"C:\\Users\\osour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openpifpaf\\transforms\\compose.py\", line 16, in __call__\n",
      "    args = p(*args)\n",
      "  File \"c:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\openpifpaf_sdaplugin\\sda.py\", line 176, in __call__\n",
      "    img = self.apply(image)\n",
      "  File \"c:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\openpifpaf_sdaplugin\\sda.py\", line 109, in apply\n",
      "    x = random.randint(0, image_width - bodypart_width)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\random.py\", line 370, in randint\n",
      "    return self.randrange(a, b+1)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\random.py\", line 353, in randrange\n",
      "    raise ValueError(\"empty range for randrange() (%d, %d, %d)\" % (istart, istop, width))\n",
      "ValueError: empty range for randrange() (0, -21, -21)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m openpifpaf.train\n",
    "    --lr=0.0003 \\\n",
    "    --momentum=0.95 \\\n",
    "    --clip-grad-value=10.0 \\\n",
    "    --b-scale=10.0 \\\n",
    "    --batch-size=8 \\\n",
    "    --loader-workers=12 \\\n",
    "    --epochs=350 \\\n",
    "    --lr-decay 310 330 \\\n",
    "    --lr-decay-epochs=10 \\\n",
    "    --val-interval 5 \\\n",
    "    --checkpoint=shufflenetv2k30 \\\n",
    "    --lr-warm-up-start-epoch=200 \\\n",
    "    --dataset=custom_animal \\\n",
    "    --custom-animal-square-edge=385 \\\n",
    "    --custom-animal-upsample=2 \\\n",
    "    --custom-animal-bmin=2 \\\n",
    "    --custom-animal-extended-scale \\\n",
    "    --weight-decay=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m openpifpaf.train  --lr=0.0003 --momentum=0.95 --clip-grad-value=10.0 --b-scale=10.0 --batch-size=8 --loader-workers=12 --epochs=350 --lr-decay 310 330 --lr-decay-epochs=10 --val-interval 5 --checkpoint=shufflenetv2k30 --lr-warm-up-start-epoch=200 --dataset=animal --animal-square-edge=385 --animal-upsample=2 --animal-bmin=2 --animal-extended-scale --weight-decay=1e-5\n",
    "\n",
    "\n",
    "\n",
    "python3 -m openpifpaf.train  --lr=0.0003 --momentum=0.95 --clip-grad-value=10.0 --b-scale=10.0 --batch-size=4 --loader-workers=12 --epochs=300 --lr-decay 310 330 --lr-decay-epochs=10 --val-interval 5 --checkpoint=custom_animal.epoch275 --lr-warm-up-start-epoch=275 --dataset=custom_animal --animal-square-edge=385 --animal-upsample=2 --animal-bmin=2 --animal-extended-scale --weight-decay=1e-5\n",
    "\n",
    "python3 -m openpifpaf.eval --dataset=custom_animal --checkpoint=outputs/paperspace/custom_animal.epoch275-230514-212322-custom_animal.pkl.epoch300 --batch-size=1 --skip-existing --loader-workers=8 --force-complete-pose --seed-threshold=0.01 --instance-threshold=0.01\n",
    "\n",
    "\n",
    "python3 -m openpifpaf.eval --dataset=custom_animal --checkpoint=shufflenetv2k30-animalpose --batch-size=1  --loader-workers=8 --force-complete-pose --seed-threshold=0.01 --instance-threshold=0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

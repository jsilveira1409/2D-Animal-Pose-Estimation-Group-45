{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpifpaf in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.13.11)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (1.13.1)\n",
      "Requirement already satisfied: torchvision==0.14.1 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (0.14.1)\n",
      "Requirement already satisfied: pillow!=8.3.0 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (9.1.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (1.23.3)\n",
      "Requirement already satisfied: importlib-metadata!=3.8.0 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (6.5.1)\n",
      "Requirement already satisfied: pysparkling in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (0.6.2)\n",
      "Requirement already satisfied: python-json-logger in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openpifpaf) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==1.13.1->openpifpaf) (4.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchvision==0.14.1->openpifpaf) (2.28.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from importlib-metadata!=3.8.0->openpifpaf) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2019.3 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pysparkling->openpifpaf) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pysparkling->openpifpaf) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.0->pysparkling->openpifpaf) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->torchvision==0.14.1->openpifpaf) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->torchvision==0.14.1->openpifpaf) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->torchvision==0.14.1->openpifpaf) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\osour\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->torchvision==0.14.1->openpifpaf) (1.26.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\osour\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "Cloning into 'plugins'...\n",
      "mv: cannot stat 'plugins/openpifpaf_animalplugin2/': No such file or directory\n",
      "mv: cannot move 'plugins/openpifpaf_sdaplugin/' to 'openpifpaf_sdaplugin/openpifpaf_sdaplugin': Directory not empty\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpifpaf_animalpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\osour\\OneDrive - epfl.ch\\EPFL\\MA2\\CIVIL-459 Deep Learning For Autonomous Vehicles\\CIVIL-459-Animal-Pose-Estimation\\test\\test.ipynb Cell 1\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/test/test.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/test/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#from openpifpaf.plugins.animalpose import AnimalKp\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/test/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenpifpaf_animalpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39manimal_kp_custom\u001b[39;00m \u001b[39mimport\u001b[39;00m AnimalKpCustom\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/test/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osour/OneDrive%20-%20epfl.ch/EPFL/MA2/CIVIL-459%20Deep%20Learning%20For%20Autonomous%20Vehicles/CIVIL-459-Animal-Pose-Estimation/test/test.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpifpaf_animalpose'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules or True:\n",
    "    !pip3 install openpifpaf\n",
    "    !rm -rf plugins\n",
    "    !rm -rf openpifpaf_animalplugin\n",
    "    !rm -rf openpifpaf_sdaplugin\n",
    "    !git clone --branch main https://github.com/jsilveira1409/CIVIL-459-Animal-Pose-Estimation.git plugins\n",
    "    !mv plugins/openpifpaf_animalpose2/ openpifpaf_animalpose\n",
    "    !mv plugins/openpifpaf_sdaplugin/ openpifpaf_sdaplugin\n",
    "    #!rm -rf plugins\n",
    "\n",
    "import openpifpaf\n",
    "from openpifpaf_sdaplugin import SDA\n",
    "import numpy as np\n",
    "\n",
    "#from openpifpaf.plugins.animalpose import AnimalKp\n",
    "from openpifpaf_animalpose.animal_kp_custom import AnimalKpCustom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import gdown\n",
    "import subprocess\n",
    "import argparse\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "openpifpaf.show.Canvas.show = True\n",
    "\n",
    "\n",
    "pred_cmd = 'python3 -m openpifpaf.predict \\\n",
    "    test.jpg \\\n",
    "    --checkpoint shufflenetv2k16 \\\n",
    "    --image-output \\\n",
    "    --json-output\\\n",
    "    --dataset=custom_animal'\n",
    "\n",
    "train_cmd = 'python3 -m openpifpaf.train \\\n",
    "    --dataset custom_animal \\\n",
    "    --basenet=shufflenetv2k16 \\\n",
    "    --lr=0.00002 \\\n",
    "    --momentum=0.95 \\\n",
    "    --epochs=1 \\\n",
    "    --lr-decay 160 260 \\\n",
    "    --lr-decay-epochs=10  \\\n",
    "    --weight-decay=1e-5 \\\n",
    "    --weight-decay=1e-5 \\\n",
    "    --val-interval 10 \\\n",
    "    --loader-workers 2 \\\n",
    "    --batch-size 1'\n",
    "\n",
    "\n",
    "\n",
    "train_annotations = 'data-animalpose/annotations/animal_keypoints_20_train.json'\n",
    "val_annotations = 'data-animalpose/annotations/animal_keypoints_20_val.json'\n",
    "eval_annotations = val_annotations\n",
    "train_image_dir = 'data-animalpose/images/train/'\n",
    "val_image_dir = 'data-animalpose/images/val/'\n",
    "eval_image_dir = val_image_dir\n",
    "\n",
    "keypoint_file = 'data-animalpose/keypoints.json'\n",
    "images_folder = 'data-animalpose/images/'\n",
    "\n",
    "def download_dataset():\n",
    "    cmd = 'rm -rf data-animalpose'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'mkdir data-animalpose'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'gdown \"https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x\" -O data-animalpose --folder'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'mkdir data-animalpose/output'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'unzip data-animalpose/images.zip -d data-animalpose/'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = 'rm data-animalpose/images.zip'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    print(\"Downloaded dataset\")\n",
    "\n",
    "def convert_keypoints_format(keypoints_list):\n",
    "    keypoints_flat = []\n",
    "    for keypoint in keypoints_list:\n",
    "        keypoints_flat.extend([k for k in keypoint])\n",
    "    return keypoints_flat\n",
    "\n",
    "def adapt_to_coco():\n",
    "    # Load the input JSON file\n",
    "    with open(keypoint_file, 'r') as f:\n",
    "        input_dict = json.load(f)\n",
    "\n",
    "    # Create a copy of the input dictionary\n",
    "    output_dict = copy.deepcopy(input_dict)\n",
    "\n",
    "    # Update images list format\n",
    "    images_list = []\n",
    "    for image_id, image_filename in input_dict['images'].items():\n",
    "        images_list.append({'id': int(image_id), 'file_name': image_filename})\n",
    "        #images_list.append({'id': image_id, image_id: image_filename})\n",
    "    output_dict['images'] = images_list\n",
    "\n",
    "    # Update annotations keypoints format and add missing fields\n",
    "    annotations_list = []\n",
    "    for i, annotation in enumerate(input_dict['annotations']):\n",
    "        new_annotation = {\n",
    "            'id': i + 1,\n",
    "            'image_id': annotation['image_id'],\n",
    "            'category_id': annotation['category_id'],\n",
    "            'bbox': annotation['bbox'],\n",
    "            'keypoints': convert_keypoints_format(annotation['keypoints']),\n",
    "            'num_keypoints': annotation['num_keypoints'],\n",
    "            #'iscrowd': 0,\n",
    "        }\n",
    "        annotations_list.append(new_annotation)\n",
    "    output_dict['annotations'] = annotations_list\n",
    "\n",
    "    # Save the converted data to a JSON file\n",
    "    with open(keypoint_file, 'w') as f:\n",
    "        json.dump(output_dict, f, indent=4)\n",
    "    print(\"Converted to COCO format\")\n",
    "\n",
    "def split_data():\n",
    "    # Load the input JSON file\n",
    "    with open(keypoint_file, 'r') as f:\n",
    "        input_dict = json.load(f)\n",
    "\n",
    "    # Create a copy of the input dictionary\n",
    "    output_dict = copy.deepcopy(input_dict)\n",
    "\n",
    "    # Split the data into train and val\n",
    "    train_dict = copy.deepcopy(output_dict)\n",
    "    val_dict = copy.deepcopy(output_dict)\n",
    "\n",
    "    # Update images list format\n",
    "    train_images_list = []\n",
    "    val_images_list = []\n",
    "    for image in output_dict['images']:\n",
    "        if random.random() < 0.8:  # 80% probability for train, 20% for validation\n",
    "            train_images_list.append(image)\n",
    "        else:\n",
    "            val_images_list.append(image)\n",
    "    train_dict['images'] = train_images_list\n",
    "    val_dict['images'] = val_images_list\n",
    "\n",
    "    # Update annotations keypoints format and add missing fields\n",
    "    train_annotations_list = []\n",
    "    val_annotations_list = []\n",
    "    for annotation in output_dict['annotations']:\n",
    "        if annotation['image_id'] in [img['id'] for img in train_images_list]:\n",
    "            train_annotations_list.append(annotation)\n",
    "        elif annotation['image_id'] in [img['id'] for img in val_images_list]:\n",
    "            val_annotations_list.append(annotation)\n",
    "    train_dict['annotations'] = train_annotations_list\n",
    "    val_dict['annotations'] = val_annotations_list\n",
    "    \n",
    "    # create files if not already existant\n",
    "    if not os.path.exists(train_image_dir):\n",
    "        os.makedirs(train_image_dir)\n",
    "    if not os.path.exists(val_image_dir):\n",
    "        os.makedirs(val_image_dir)\n",
    "    if not os.path.exists(eval_image_dir):\n",
    "        os.makedirs(eval_image_dir)\n",
    "    if not os.path.exists('data-animalpose/annotations/'):\n",
    "        os.makedirs('data-animalpose/annotations/')\n",
    "\n",
    "    # create train_annotations file\n",
    "    if not os.path.exists(train_annotations):\n",
    "        open(train_annotations, 'w').close()\n",
    "    # create val_annotations file\n",
    "    if not os.path.exists(val_annotations):\n",
    "        open(val_annotations, 'w').close()\n",
    "    # create eval_annotations file\n",
    "    if not os.path.exists(eval_annotations):\n",
    "        open(eval_annotations, 'w').close()\n",
    "\n",
    "        \n",
    "    # Save the converted data to a JSON file\n",
    "    \n",
    "    with open(train_annotations, 'w') as f:\n",
    "        json.dump(train_dict, f, indent=4)\n",
    "    with open(eval_annotations, 'w') as f:\n",
    "        json.dump(val_dict, f, indent=4)\n",
    "\n",
    "\n",
    "    # move images to train and val folders\n",
    "    for image in train_images_list:\n",
    "        #file_name = image[str(image['id'])]\n",
    "        file_name = image['file_name']\n",
    "        os.rename(images_folder + file_name, train_image_dir + file_name)\n",
    "    for image in val_images_list:\n",
    "        #file_name = image[str(image['id'])]\n",
    "        file_name = image['file_name']\n",
    "        os.rename(images_folder + file_name, val_image_dir + file_name)\n",
    "    print(\"Split data into train and val\")\n",
    "\n",
    "def test_sda (sda):\n",
    "    img = Image.open('data-animalpose/images/train/2007_001397.jpg')\n",
    "    tensor_img = np.array(img)\n",
    "    img1 = sda.apply(tensor_img)\n",
    "    plt.imshow(tensor_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. Download dataset\n",
    "download_dataset()\n",
    "# 2. Convert to COCO format \n",
    "adapt_to_coco()\n",
    "# 3. Split data into train and val\n",
    "split_data()\n",
    "\n",
    "# 4. Initialize SDA and crop the dataset, creating a body part pool\n",
    "sda = SDA()\n",
    "sda.crop_dataset()\n",
    "print(\"Cropped dataset\")\n",
    "\n",
    "# 5. Configure plugins\n",
    "config = openpifpaf.plugin.register()\n",
    "print(openpifpaf.DATAMODULES)\n",
    "\n",
    "# 6. Train the model\n",
    "subprocess.run(train_cmd, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
